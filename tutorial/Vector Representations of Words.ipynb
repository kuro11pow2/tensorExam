{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Representations of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이번시간에는 word2vec에 대해 해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 왜 word2vec을 사용해야하나\n",
    "- 어떻게 훈련시킬까\n",
    "- TensorFlow로 간단한 구현을 해보자.\n",
    "- 더좋은 방법도 찾아보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "코드는 아래에있다.\n",
    "\n",
    "[간단한버전](https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/examples/tutorials/word2vec/word2vec_basic.py)\n",
    "\n",
    "\n",
    "[좀어려운버전](https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow_models/tutorials/embedding/word2vec.py) (이거 페이지를 찾을수가없다고나온다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings를 배워야 하는 이유"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예전에는 개와 고양이 같은경우 discrete atomic symbols로 구분했었다. 예를들어 고양이는 Id537이고, 개는 Id143으로 구분했었다.\n",
    "\n",
    "이런 인코딩은 임의로 아이디값을 부여해서 별로연관성이없고. 각 단어의 이미의 관계를 찾을수가없었다.\n",
    "\n",
    "개와 고양이는 유사한점이 상당히 많지만, symbols값만 보고는 구분할수가없었다.\n",
    "\n",
    "하지만 vector로 표현해서 진행하면 이러한 단점을 어느정도 극복할수가있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector space models(VSM)은 word들을 비슷한단어끼리 mapping시켜준다.\n",
    "\n",
    "같은 문장에서 같은의미를 갖는 Distributional Hypothesis란 방법을 이용한 것은 똑같다.\n",
    "\n",
    "이방법은 크게 __count-based 방법(잠재 의미론적방법)__ 과 __predictive 방법(신경망 확률 론적 언어모델)__ 으로 나눌수있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count-based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count based 방법은 큰 말뭉치에서 인접단어들이 얼마나 자주 나타나는지에 대한 통계를 계산한다음, 각단어에 대한 작고, dense한 벡터로 매핑합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictive Models는 주변의 작고, 밀집된 단어들로부터 word를 예측해낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW 와 Skip-Gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__CBOW__와 __Skip-Gram__은 많이 비슷하다. 하지만,\n",
    "하지만, CBOW는 source words에서 target words를 예측하지만, Skip-Gram은 반대이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CBOW는 통계적으로 볼때 많은 분포정보에 대해 좀더 매끄럽게 작동한다.\n",
    "작은 데이터셋에서 유용하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip-Gram은 context-target쌍을 새로운 관측으로 취급하기때문에, 큰데이터셋에서 유용하다. 이번 튜토리얼에서는 Skip-gram에 대해 더 초점을 맞출예정이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling up with Noise-Contrastive Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전에 단어 *h(history)* 를 이용해서 다음단어 *w*를 예상한다 \n",
    "\n",
    "하지만, \n",
    "\n",
    "이방법은 모든 Vector에서 *w*를 현재 context *h* 에서  계산하여서 비용 굉장히 많이든다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그래서"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$$\\sum_{i=0}^n i^2 = \\frac{(n^2+n)(2n+1)}{6}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$$P(w_{t}|)$$\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

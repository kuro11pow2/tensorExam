{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import datetime\n",
    "from glob import iglob\n",
    "import time\n",
    "import attention\n",
    "from collections import deque\n",
    "import pickle\n",
    "import pyreader\n",
    "import numpy as np\n",
    "import csv\n",
    "from batchmake import Batcher\n",
    "import pprint\n",
    "tf.set_random_seed(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data_samples/mapping.map'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-ab3ab0a1f290>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#         vocabs.append(word)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mword_to_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'data_samples/mapping.map'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_to_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mword_to_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0minv_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mk\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_to_id\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data_samples/mapping.map'"
     ]
    }
   ],
   "source": [
    "datas= 'data_samples/'\n",
    "# vocabulary = 'vocab.csv'\n",
    "# vocabs = []\n",
    "# with open(vocabulary, 'r', newline='', encoding='utf-8') as vocab:\n",
    "#     words = csv.reader(vocab)\n",
    "#     for word in words:\n",
    "#         vocabs.append(word)\n",
    "word_to_id='data_samples/mapping.map'\n",
    "with open(word_to_id, 'rb') as f:\n",
    "    word_to_id = pickle.load(f)\n",
    "inv_map = {v:k for k, v in word_to_id.items() }\n",
    "# word_to_id\n",
    "# inv_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def attention(inputs):\n",
    "    \n",
    "    # inputs = Batch * hidden\n",
    "    inputs_shape = inputs.shape\n",
    "#     sequence_length = inputs_shape[1].value\n",
    "    hidden_size = inputs_shape[1].value\n",
    "    \n",
    "    w_omega = tf.Variable(tf.random_normal([hidden_size,attention_size], stddev=0.1))\n",
    "    b_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
    "    u_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
    "    \n",
    "    # batch *k , attention_size\n",
    "    v = tf.tanh(tf.matmul(tf.reshape(inputs, [-1,hidden_size]), w_omega)+\n",
    "               tf.reshape(b_omega,[1,-1]))\n",
    "    \n",
    "    vu = tf.matmul(v, tf.reshape(u_omega,[-1,1])) # batch*k,1\n",
    "    \n",
    "    exps = tf.reshape(tf.exp(vu), [-1, sequence])  # (batch, k)\n",
    "    alphas = exps / tf.reshape(tf.reduce_sum(exps,1), [-1,1])\n",
    "    #(batch,k)\n",
    "    output = tf.reduce_sum(inputs * tf.reshape(alphas, [-1, sequence,1]),1)\n",
    "    # batch * sequence * hidden -> batch * hidden\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_attention(inputs, seq_len, initial_state):\n",
    "    \n",
    "    state = initial_state\n",
    "    outputs = []\n",
    "    states = []\n",
    "    with tf.variable_scope(\"RNN\"):\n",
    "        time = tf.constant(0)\n",
    "        for time_step in  range(seq_len):\n",
    "            if time_step>0:\n",
    "                tf.get_variable_scope().reuse_variables()\n",
    "            print('time_step', inputs[:,time_step,:])\n",
    "            output = attention(inputs[:,time_step,:])\n",
    "            outputs.append(output)\n",
    "            states.append(state)\n",
    "            \n",
    "        output_tensor = tf.stack(outputs)\n",
    "        #output_tensor : time_step * \n",
    "        \n",
    "    return output_tensor, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"embedding_lookup:0\", shape=(?, 5, 50), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "hidden_size=32\n",
    "sequence=5\n",
    "embedding_dim=50\n",
    "attention_size = 50\n",
    "batch_size=16\n",
    "vocab_size=len(word_to_id)\n",
    "grucell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size)\n",
    "X = tf.placeholder(tf.int32, [None, sequence], name='inputs_xdata')\n",
    "# Y = tf.placeholder(tf.float32, [None, sequence], name='targets_ydata')\n",
    "Y = tf.placeholder(tf.int32, [None, sequence], name='targets_ydata')\n",
    "seq_len = tf.placeholder(tf.int32, [None], name='seq_len')\n",
    "\n",
    "embedding_variable = tf.Variable(tf.random_uniform([vocab_size, embedding_dim],-1.0,1.0), trainable=True)\n",
    "# inputs_ = tf.transpose(X)\n",
    "batch_embedded = tf.nn.embedding_lookup(embedding_variable, X)\n",
    "\n",
    "print(batch_embedded)\n",
    "with tf.variable_scope('RNN'):\n",
    "    outputs, states = tf.nn.dynamic_rnn(grucell,\n",
    "                                        inputs=batch_embedded,\n",
    "                                        sequence_length=seq_len,\n",
    "                                        dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initial_state = grucell.zero_state(batch_size, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"strided_slice_10:0\", shape=(?, 32), dtype=float32)\n",
      "time_step Tensor(\"RNN_15/strided_slice:0\", shape=(?, 32), dtype=float32)\n",
      "time_step Tensor(\"RNN_15/strided_slice_2:0\", shape=(?, 32), dtype=float32)\n",
      "time_step Tensor(\"RNN_15/strided_slice_4:0\", shape=(?, 32), dtype=float32)\n",
      "time_step Tensor(\"RNN_15/strided_slice_6:0\", shape=(?, 32), dtype=float32)\n",
      "time_step Tensor(\"RNN_15/strided_slice_8:0\", shape=(?, 32), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'RNN_15/stack:0' shape=(5, ?, 32) dtype=float32>,\n",
       " LSTMStateTuple(c=<tf.Tensor 'BasicLSTMCellZeroState_1/zeros:0' shape=(16, 32) dtype=float32>, h=<tf.Tensor 'BasicLSTMCellZeroState_1/zeros_1:0' shape=(16, 32) dtype=float32>))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(outputs[:,3,:])\n",
    "make_attention(outputs, sequence, initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Sum_1:0' shape=(?, 32) dtype=float32>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_output = attention(outputs, attention_size=attention_size)\n",
    "attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = tf.reshape(Y, [batch_size*sequence, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14760\n"
     ]
    }
   ],
   "source": [
    "keep_prob = 0.5\n",
    "drop = tf.nn.dropout(attention_output, keep_prob=keep_prob)\n",
    "w = tf.Variable(tf.truncated_normal([hidden_size,vocab_size], stddev=0.1))\n",
    "b = tf.Variable(tf.constant(0., shape=[vocab_size]))\n",
    "y_hat = tf.nn.xw_plus_b(drop,w,b)\n",
    "\n",
    "# y_hat = tf.squeeze(y_hat)\n",
    "\n",
    "# loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "#     logits=y_hat,\n",
    "#     labels=Y))\n",
    "# loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y_hat, labels=Y)\n",
    "labels = tf.reshape(Y, [-1], name='label_reshape')\n",
    "# print(y_hat, Y, labels, tf.transpose(w))\n",
    "print(vocab_size)\n",
    "\n",
    "loss = tf.nn.sampled_softmax_loss(weights=tf.transpose(w), biases=b, labels=Y,\n",
    "                                  inputs=attention_output, num_sampled = 100, num_classes=vocab_size\n",
    "                                 )\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accuracy 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# accuracy = 1. - tf.reduce_mean(tf.cast(tf.equal(tf.round(y_hat), Y), tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_name_queue = tf.train.string_input_producer(['inputs'])\n",
    "reader = tf.TFRecordReader()\n",
    "_, single_x = reader.read(file_name_queue)\n",
    "\n",
    "context_features = {\n",
    "    \"length\": tf.FixedLenFeature([], dtype=tf.int64)\n",
    "}\n",
    "sequence_features = {\n",
    "    \"tokens\": tf.FixedLenSequenceFeature([], dtype=tf.int64),\n",
    "    \"labels\": tf.FixedLenSequenceFeature([], dtype=tf.int64)\n",
    "}\n",
    "\n",
    "\n",
    "context_parsed, sequence_parsed = tf.parse_single_sequence_example(serialized=single_x,\n",
    "                                 context_features=context_features,\n",
    "                                 sequence_features=sequence_features\n",
    "                                )\n",
    "batch_seq, batch_x, batch_y = tf.train.batch(tensors=[context_parsed['length'],sequence_parsed['tokens'],sequence_parsed['labels']],\n",
    "                                                     batch_size=batch_size, \n",
    "                                                     capacity=5000,\n",
    "                                                     num_threads=1,\n",
    "                                             dynamic_pad=True\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start learning!\n",
      "0 start\n",
      "x_in.shape, seq.shape (16, 5) (16,)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Incompatible shapes: [16,1,32] vs. [80,1,32]\n\t [[Node: gradients/sampled_softmax_loss_2/Mul_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](gradients/sampled_softmax_loss_2/Mul_grad/Shape, gradients/sampled_softmax_loss_2/Mul_grad/Shape_1)]]\n\nCaused by op 'gradients/sampled_softmax_loss_2/Mul_grad/BroadcastGradientArgs', defined at:\n  File \"D:\\python3_5anaconda\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"D:\\python3_5anaconda\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-13-4664bd4b526a>\", line 21, in <module>\n    optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(loss)\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 315, in minimize\n    grad_loss=grad_loss)\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 386, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 540, in gradients\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 346, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 540, in <lambda>\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py\", line 663, in _MulGrad\n    rx, ry = gen_array_ops._broadcast_gradient_args(sx, sy)\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 395, in _broadcast_gradient_args\n    name=name)\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\n...which was originally created as op 'sampled_softmax_loss_2/Mul', defined at:\n  File \"D:\\python3_5anaconda\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 18 identical lines from previous traceback]\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-13-4664bd4b526a>\", line 18, in <module>\n    inputs=attention_output, num_sampled = 100, num_classes=vocab_size\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py\", line 1268, in sampled_softmax_loss\n    name=name)\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py\", line 1007, in _compute_sampled_logits\n    array_ops.reshape(true_w, new_true_w_shape))\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 286, in multiply\n    return gen_math_ops._mul(x, y, name)\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 1377, in _mul\n    result = _op_def_lib.apply_op(\"Mul\", x=x, y=y, name=name)\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): Incompatible shapes: [16,1,32] vs. [80,1,32]\n\t [[Node: gradients/sampled_softmax_loss_2/Mul_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](gradients/sampled_softmax_loss_2/Mul_grad/Shape, gradients/sampled_softmax_loss_2/Mul_grad/Shape_1)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mD:\\python3_5anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python3_5anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python3_5anaconda\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python3_5anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [16,1,32] vs. [80,1,32]\n\t [[Node: gradients/sampled_softmax_loss_2/Mul_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](gradients/sampled_softmax_loss_2/Mul_grad/Shape, gradients/sampled_softmax_loss_2/Mul_grad/Shape_1)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-0220afe96e7d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'x_in.shape, seq.shape'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_in\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m#         lo, acc, opt = Ses.run([loss, accuracy, optimizer], feed_dict=feed_data)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mlo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;31m#         accuracy_train+=acc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mloss_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlo\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdelta\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss_train\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python3_5anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python3_5anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\python3_5anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mD:\\python3_5anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1150\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1152\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [16,1,32] vs. [80,1,32]\n\t [[Node: gradients/sampled_softmax_loss_2/Mul_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](gradients/sampled_softmax_loss_2/Mul_grad/Shape, gradients/sampled_softmax_loss_2/Mul_grad/Shape_1)]]\n\nCaused by op 'gradients/sampled_softmax_loss_2/Mul_grad/BroadcastGradientArgs', defined at:\n  File \"D:\\python3_5anaconda\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"D:\\python3_5anaconda\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-13-4664bd4b526a>\", line 21, in <module>\n    optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(loss)\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 315, in minimize\n    grad_loss=grad_loss)\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 386, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 540, in gradients\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 346, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py\", line 540, in <lambda>\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py\", line 663, in _MulGrad\n    rx, ry = gen_array_ops._broadcast_gradient_args(sx, sy)\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 395, in _broadcast_gradient_args\n    name=name)\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\n...which was originally created as op 'sampled_softmax_loss_2/Mul', defined at:\n  File \"D:\\python3_5anaconda\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 18 identical lines from previous traceback]\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-13-4664bd4b526a>\", line 18, in <module>\n    inputs=attention_output, num_sampled = 100, num_classes=vocab_size\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py\", line 1268, in sampled_softmax_loss\n    name=name)\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py\", line 1007, in _compute_sampled_logits\n    array_ops.reshape(true_w, new_true_w_shape))\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 286, in multiply\n    return gen_math_ops._mul(x, y, name)\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 1377, in _mul\n    result = _op_def_lib.apply_op(\"Mul\", x=x, y=y, name=name)\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"D:\\python3_5anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): Incompatible shapes: [16,1,32] vs. [80,1,32]\n\t [[Node: gradients/sampled_softmax_loss_2/Mul_grad/BroadcastGradientArgs = BroadcastGradientArgs[T=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](gradients/sampled_softmax_loss_2/Mul_grad/Shape, gradients/sampled_softmax_loss_2/Mul_grad/Shape_1)]]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "delta = 0.5\n",
    "\n",
    "Ses = tf.Session()\n",
    "\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=Ses, coord=coord)\n",
    "Ses.run(tf.global_variables_initializer())\n",
    "\n",
    "print('start learning!')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    print('{} start'.format(epoch))\n",
    "    \n",
    "    loss_train = 0\n",
    "    loss_test = 0\n",
    "    accuracy_train = 0\n",
    "    accuracy_test =0\n",
    "    \n",
    "    \n",
    "#     for i in range(1000):\n",
    "    i=0\n",
    "#     try:\n",
    "    while True:\n",
    "        seq_, x_in, y_in = Ses.run([batch_seq, batch_x, batch_y])\n",
    "#         print('y_in', np.reshape(y_in,[-1,1]))\n",
    "        y_in = np.reshape(y_in,[-1,1])\n",
    "        feed_data = {X:x_in, Y:y_in, seq_len:seq_}\n",
    "        print('x_in.shape, seq.shape', x_in.shape, seq_.shape)\n",
    "#         lo, acc, opt = Ses.run([loss, accuracy, optimizer], feed_dict=feed_data)\n",
    "        lo, opt = Ses.run([loss, optimizer], feed_dict=feed_data)\n",
    "#         accuracy_train+=acc\n",
    "        loss_train = lo * delta + loss_train * (1-delta)\n",
    "        if i%100 ==0:\n",
    "            print(' {} : loss {} acc {}'.format(i, lo, accuracy_train))\n",
    "#     except:\n",
    "#         print('batch : ',i)\n",
    "#         accuracy_train /="
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

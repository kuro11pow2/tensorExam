{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import datetime\n",
    "from glob import iglob\n",
    "import time\n",
    "# import attention\n",
    "from collections import deque\n",
    "import pickle\n",
    "import pyreader\n",
    "import numpy as np\n",
    "import csv\n",
    "from batchmake import Batcher\n",
    "import pprint\n",
    "tf.set_random_seed(777)\n",
    "from tensorflow.python.layers import core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "datas= 'data_samples/'\n",
    "vocabulary = 'vocab.csv'\n",
    "vocabs = []\n",
    "with open(vocabulary, 'r', newline='', encoding='utf-8') as vocab:\n",
    "    words = csv.reader(vocab)\n",
    "    for i, word in enumerate(words):\n",
    "        vocabs.append(word[0])\n",
    "print(len(vocabs))\n",
    "word_to_id = {word: i for i, word in enumerate(vocabs)}\n",
    "id_to_word = {i:word for i, word in enumerate(vocabs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<PAD>', 0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_word[0], word_to_id['<PAD>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_size=100\n",
    "sequence=5\n",
    "embedding_dim=50\n",
    "attention_size = 50\n",
    "batch_size=8\n",
    "vocab_size=len(vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model_inputs():\n",
    "    X = tf.placeholder(tf.int32, [None, sequence], name='inputs_xdata')\n",
    "    # Y = tf.placeholder(tf.float32, [None, sequence], name='targets_ydata')\n",
    "    Y = tf.placeholder(tf.int32, [None, sequence], name='targets_ydata')\n",
    "#     seq_len = tf.placeholder(tf.int32, [None], name='seq_len')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "\n",
    "#     embedding_variable = tf.Variable(tf.random_uniform([vocab_size, embedding_dim],-1.0,1.0), trainable=True)\n",
    "#     batch_embedded = tf.nn.embedding_lookup(embedding_variable, X)\n",
    "    target_sequence_length = tf.placeholder(tf.int32, (None,), name='target_sequence_length')\n",
    "    max_target_sequence_length = tf.reduce_max(target_sequence_length, name='max_target_len')\n",
    "    source_sequence_length = tf.placeholder(tf.int32, (None,), name='source_sequence_length')\n",
    "    \n",
    "    return X, Y, lr, target_sequence_length, source_sequence_length, max_target_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoding_layer(input_data, rnn_size, num_layers,\n",
    "                   source_sequence_length, source_vocab_size, \n",
    "                   encoding_embedding_size):\n",
    "    # Encoder embedding\n",
    "    enc_embed_input = tf.contrib.layers.embed_sequence(input_data, source_vocab_size, encoding_embedding_size)\n",
    "\n",
    "    # RNN cell\n",
    "    def make_cell(rnn_size):\n",
    "        enc_cell = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                           initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "        return enc_cell\n",
    "\n",
    "    enc_cell = tf.contrib.rnn.MultiRNNCell([make_cell(rnn_size) for _ in range(num_layers)])\n",
    "    \n",
    "    enc_output, enc_state = tf.nn.dynamic_rnn(enc_cell, enc_embed_input, sequence_length=source_sequence_length, dtype=tf.float32)\n",
    "    \n",
    "    return enc_output, enc_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_decoder_input(target_data, word_to_int, batch_size):\n",
    "    ending = tf.strided_slice(target_data, [0,0], [batch_size, -1], [1, 1])\n",
    "    dec_input = tf.concat([tf.fill([batch_size,1], word_to_int['<GO>']), ending], 1)\n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoding_layer(target_letter_to_int, decoding_embedding_size, num_layers, rnn_size,\n",
    "                   target_sequence_length, max_target_sequence_length, enc_state, dec_input):\n",
    "    \n",
    "    target_vocab_size = len(target_letter_to_int)\n",
    "    \n",
    "    dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))\n",
    "\n",
    "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
    "\n",
    "    def make_cell(rnn_size):\n",
    "        dec_cell = tf.contrib.rnn.LSTMCell(rnn_size, initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "        return dec_cell\n",
    "\n",
    "    dec_cell = tf.contrib.rnn.MultiRNNCell([make_cell(rnn_size) for _ in range(num_layers)])\n",
    "\n",
    "    output_layer = core.Dense(target_vocab_size,\n",
    "                              kernel_initializer=tf.truncated_normal_initializer(mean = 0.0, stddev=0.1)\n",
    "                             )\n",
    "    \n",
    "    with tf.variable_scope('decode'):\n",
    "        training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
    "                                                            sequence_length = target_sequence_length,\n",
    "                                                            time_major=False\n",
    "                                                           )\n",
    "        \n",
    "        training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                           training_helper,\n",
    "                                                           enc_state,\n",
    "                                                           output_layer\n",
    "                                                          )\n",
    "        \n",
    "        training_decoder_output,final_state, final_sequence_lengths = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                                                                                        impute_finished=True,\n",
    "                                                                                                        maximum_iterations=max_target_sequence_length\n",
    "                                                                                                       )\n",
    "        \n",
    "    with tf.variable_scope('decode', reuse=True):\n",
    "        start_tokens = tf.tile(tf.constant([target_letter_to_int['<GO>']],dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "        inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_embeddings,\n",
    "                                                                    start_tokens,\n",
    "                                                                    target_letter_to_int['<EOS>']\n",
    "                                                                   )\n",
    "        \n",
    "        inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, \n",
    "                                                            inference_helper,\n",
    "                                                            enc_state,\n",
    "                                                            output_layer\n",
    "                                                           )\n",
    "        \n",
    "        inference_decoder_output, final_state, final_sequence_lengths = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                                                                                          impute_finished=True,\n",
    "                                                                                                          maximum_iterations=max_target_sequence_length\n",
    "                                                                                                         )\n",
    "        \n",
    "        print('training_decoder_output.rnn_output, inference_decoder_output.sample_id', training_decoder_output.rnn_output, inference_decoder_output.sample_id)\n",
    "        print('training_decoder_output, inference_decoder_output', training_decoder_output, inference_decoder_output)\n",
    "    return training_decoder_output, inference_decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, targets, target_sequence_length,max_target_sequence_length,\n",
    "                  source_sequence_length,source_vocab_size, target_vocab_size, enc_embedding_size, dec_embedding_size,\n",
    "                  rnn_size, num_layers\n",
    "                 ):\n",
    "\n",
    "    _, enc_state = encoding_layer(input_data=input_data, rnn_size=rnn_size, num_layers=num_layers,\n",
    "                                  source_sequence_length = source_sequence_length,\n",
    "                                  source_vocab_size = source_vocab_size,\n",
    "                                  encoding_embedding_size=embedding_dim\n",
    "                                 )\n",
    "    \n",
    "    dec_input = process_decoder_input(targets, word_to_id, batch_size)\n",
    "\n",
    "    training_decoder_output, inference_decoder_output = decoding_layer(target_letter_to_int=word_to_id,\n",
    "                                                                       decoding_embedding_size=embedding_dim,\n",
    "                                                                       num_layers = num_layers,\n",
    "                                                                       rnn_size=rnn_size,\n",
    "                                                                       target_sequence_length=target_sequence_length,\n",
    "                                                                       max_target_sequence_length=max_target_sequence_length,\n",
    "                                                                       enc_state=enc_state,\n",
    "                                                                       dec_input=dec_input\n",
    "                                                                      )\n",
    "    \n",
    "    return training_decoder_output,inference_decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data fetched \n",
      "training_decoder_output.rnn_output, inference_decoder_output.sample_id Tensor(\"decode/decoder/transpose:0\", shape=(8, ?, 50000), dtype=float32) Tensor(\"decode_1/decoder/transpose_1:0\", shape=(8, ?), dtype=int32)\n",
      "training_decoder_output, inference_decoder_output BasicDecoderOutput(rnn_output=<tf.Tensor 'decode/decoder/transpose:0' shape=(8, ?, 50000) dtype=float32>, sample_id=<tf.Tensor 'decode/decoder/transpose_1:0' shape=(8, ?) dtype=int32>) BasicDecoderOutput(rnn_output=<tf.Tensor 'decode_1/decoder/transpose:0' shape=(8, ?, 50000) dtype=float32>, sample_id=<tf.Tensor 'decode_1/decoder/transpose_1:0' shape=(8, ?) dtype=int32>)\n",
      "seq2seq model created! \n",
      "optmization created!\n"
     ]
    }
   ],
   "source": [
    "train_graph = tf.Graph()\n",
    "# Set the graph to default to ensure that it is ready for training\n",
    "with train_graph.as_default():\n",
    "\n",
    "    # Load the model inputs    \n",
    "    input_data, targets, lr, target_sequence_length, source_sequence_length, max_target_sequence_length = get_model_inputs()\n",
    "    print('input_data fetched ')\n",
    "    # Create the training and inference logits\n",
    "    training_decoder_output, inference_decoder_output = seq2seq_model(input_data=input_data, \n",
    "                                                                      targets=targets, \n",
    "                                                                      target_sequence_length = target_sequence_length, \n",
    "                                                                      max_target_sequence_length = max_target_sequence_length, \n",
    "                                                                      source_sequence_length = source_sequence_length,\n",
    "                                                                      source_vocab_size = len(word_to_id),\n",
    "                                                                      target_vocab_size = len(word_to_id),\n",
    "                                                                      enc_embedding_size = embedding_dim, \n",
    "                                                                      dec_embedding_size = embedding_dim, \n",
    "                                                                      rnn_size = hidden_size, \n",
    "                                                                      num_layers = 1)\n",
    "    \n",
    "    print('seq2seq model created! ')\n",
    "    # Create tensors for the training logits and inference logits\n",
    "    training_logits = tf.identity(training_decoder_output.rnn_output, 'logits')\n",
    "    predictions_sample_id = tf.identity(inference_decoder_output.sample_id, name='predictions_sample_id')\n",
    "    predictions_output = tf.identity(inference_decoder_output.rnn_output, name='predictions_output')\n",
    "\n",
    "    # Create the weights for sequence_loss\n",
    "    masks = tf.sequence_mask(target_sequence_length, max_target_sequence_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "\n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(training_logits, targets, masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "        print('optmization created!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session !\n",
      "gogo~\n",
      "epoch 1\n",
      "1 loss 10.858777046203613 \n",
      "2 loss 10.816905975341797 \n",
      "3 loss 10.837102890014648 \n",
      "4 loss 10.828079223632812 \n",
      "5 loss 10.8179349899292 \n",
      "6 loss 10.833903312683105 \n",
      "7 loss 10.806513786315918 \n",
      "8 loss 10.794965744018555 \n",
      "9 loss 10.786519050598145 \n",
      "10 loss 10.805580139160156 \n",
      "11 loss 10.797948837280273 \n",
      "12 loss 10.807937622070312 \n",
      "13 loss 10.795125961303711 \n",
      "14 loss 10.77406120300293 \n",
      "15 loss 10.808503150939941 \n",
      "16 loss 10.77070140838623 \n",
      "17 loss 10.76120662689209 \n",
      "18 loss 10.767512321472168 \n",
      "19 loss 10.772180557250977 \n",
      "20 loss 10.775230407714844 \n",
      "21 loss 10.752634048461914 \n",
      "22 loss 10.736244201660156 \n",
      "23 loss 10.742671012878418 \n",
      "24 loss 10.780303001403809 \n",
      "25 loss 10.720174789428711 \n",
      "26 loss 10.731595993041992 \n",
      "27 loss 10.723150253295898 \n",
      "28 loss 10.749125480651855 \n",
      "29 loss 10.745424270629883 \n",
      "30 loss 10.711217880249023 \n",
      "31 loss 10.71422290802002 \n",
      "32 loss 10.708455085754395 \n",
      "33 loss 10.70934009552002 \n",
      "34 loss 10.68932819366455 \n",
      "35 loss 10.699453353881836 \n",
      "36 loss 10.699026107788086 \n",
      "37 loss 10.729511260986328 \n",
      "38 loss 10.721830368041992 \n",
      "39 loss 10.676759719848633 \n",
      "40 loss 10.702104568481445 \n",
      "41 loss 10.681177139282227 \n",
      "42 loss 10.688329696655273 \n",
      "43 loss 10.656614303588867 \n",
      "44 loss 10.676321029663086 \n",
      "45 loss 10.63353157043457 \n",
      "46 loss 10.656835556030273 \n",
      "47 loss 10.660490989685059 \n",
      "48 loss 10.633450508117676 \n",
      "49 loss 10.622344970703125 \n",
      "50 loss 10.633038520812988 \n",
      "51 loss 10.640853881835938 \n",
      "52 loss 10.61430835723877 \n",
      "53 loss 10.680760383605957 \n",
      "54 loss 10.691957473754883 \n",
      "55 loss 10.744941711425781 \n",
      "56 loss 10.696284294128418 \n",
      "57 loss 10.716363906860352 \n",
      "58 loss 10.710248947143555 \n",
      "59 loss 10.613759994506836 \n",
      "60 loss 10.59244441986084 \n",
      "61 loss 10.62173843383789 \n",
      "62 loss 10.6393404006958 \n",
      "63 loss 10.625463485717773 \n",
      "64 loss 10.56726360321045 \n",
      "65 loss 10.567535400390625 \n",
      "66 loss 10.556941986083984 \n",
      "67 loss 10.557241439819336 \n",
      "68 loss 10.522567749023438 \n",
      "69 loss 10.537186622619629 \n",
      "70 loss 10.58172607421875 \n",
      "71 loss 10.528973579406738 \n",
      "72 loss 10.515204429626465 \n",
      "73 loss 10.50157642364502 \n",
      "74 loss 10.538629531860352 \n",
      "75 loss 10.517081260681152 \n",
      "76 loss 10.49131965637207 \n",
      "77 loss 10.52367115020752 \n",
      "78 loss 10.532086372375488 \n",
      "79 loss 10.470891952514648 \n",
      "80 loss 10.61627197265625 \n",
      "81 loss 10.490423202514648 \n",
      "82 loss 10.494321823120117 \n",
      "83 loss 10.505457878112793 \n",
      "84 loss 10.454874038696289 \n",
      "85 loss 10.45713996887207 \n",
      "86 loss 10.446003913879395 \n",
      "87 loss 10.398176193237305 \n",
      "88 loss 10.432907104492188 \n",
      "89 loss 10.388895034790039 \n",
      "90 loss 10.46939468383789 \n",
      "91 loss 10.453577995300293 \n",
      "92 loss 10.441408157348633 \n",
      "93 loss 10.479070663452148 \n",
      "94 loss 10.390454292297363 \n",
      "95 loss 10.46606731414795 \n",
      "96 loss 10.3396577835083 \n",
      "97 loss 10.353506088256836 \n",
      "98 loss 10.288309097290039 \n",
      "99 loss 10.257784843444824 \n",
      "100 loss 10.400569915771484 \n",
      "101 loss 10.23614501953125 \n",
      "102 loss 10.342683792114258 \n",
      "103 loss 10.287431716918945 \n",
      "104 loss 10.283013343811035 \n",
      "105 loss 10.145360946655273 \n",
      "106 loss 10.297857284545898 \n",
      "107 loss 10.3035888671875 \n",
      "108 loss 10.171265602111816 \n",
      "109 loss 10.240211486816406 \n",
      "110 loss 10.11717414855957 \n",
      "111 loss 10.13518238067627 \n",
      "112 loss 10.071717262268066 \n",
      "113 loss 10.12018871307373 \n",
      "114 loss 10.025642395019531 \n",
      "115 loss 10.03833293914795 \n",
      "116 loss 10.020852088928223 \n",
      "117 loss 10.017666816711426 \n",
      "118 loss 9.989651679992676 \n",
      "119 loss 9.913469314575195 \n",
      "120 loss 10.290903091430664 \n",
      "121 loss 10.257549285888672 \n",
      "122 loss 10.140721321105957 \n",
      "123 loss 9.91261100769043 \n",
      "124 loss 9.902826309204102 \n",
      "125 loss 9.996125221252441 \n",
      "126 loss 9.94105339050293 \n",
      "127 loss 9.814959526062012 \n",
      "128 loss 9.768125534057617 \n",
      "129 loss 9.920138359069824 \n",
      "130 loss 9.890130996704102 \n",
      "131 loss 9.783571243286133 \n",
      "132 loss 9.65922737121582 \n",
      "133 loss 9.699999809265137 \n",
      "134 loss 9.739567756652832 \n",
      "135 loss 9.624013900756836 \n",
      "136 loss 9.815333366394043 \n",
      "137 loss 9.749448776245117 \n",
      "138 loss 9.699902534484863 \n",
      "139 loss 9.633861541748047 \n",
      "140 loss 9.5458984375 \n",
      "141 loss 9.444430351257324 \n",
      "142 loss 9.681347846984863 \n",
      "143 loss 9.636909484863281 \n",
      "144 loss 9.497981071472168 \n",
      "145 loss 9.599306106567383 \n",
      "146 loss 9.539996147155762 \n",
      "147 loss 9.592283248901367 \n",
      "148 loss 9.541959762573242 \n",
      "149 loss 9.421221733093262 \n",
      "150 loss 9.3706636428833 \n",
      "151 loss 9.216131210327148 \n",
      "152 loss 9.319669723510742 \n",
      "153 loss 9.399178504943848 \n",
      "154 loss 9.596593856811523 \n",
      "155 loss 9.10998821258545 \n",
      "156 loss 9.153351783752441 \n",
      "157 loss 9.146028518676758 \n",
      "158 loss 8.938020706176758 \n",
      "159 loss 9.17644214630127 \n",
      "160 loss 9.01738166809082 \n",
      "161 loss 8.820072174072266 \n",
      "162 loss 8.973661422729492 \n",
      "163 loss 8.894071578979492 \n",
      "164 loss 8.874226570129395 \n",
      "165 loss 8.800548553466797 \n",
      "166 loss 8.842249870300293 \n",
      "167 loss 9.011648178100586 \n",
      "168 loss 8.913497924804688 \n",
      "169 loss 8.8359956741333 \n",
      "170 loss 9.435100555419922 \n",
      "171 loss 9.0520601272583 \n",
      "172 loss 8.840550422668457 \n",
      "173 loss 8.575819969177246 \n",
      "174 loss 8.514135360717773 \n",
      "175 loss 8.413228034973145 \n",
      "176 loss 8.555392265319824 \n",
      "177 loss 8.565207481384277 \n",
      "178 loss 8.4669771194458 \n",
      "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.CancelledError'>, Enqueue operation was cancelled\n",
      "\t [[Node: input_producer/input_producer_EnqueueMany = QueueEnqueueManyV2[Tcomponents=[DT_STRING], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_producer, input_producer/RandomShuffle)]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-819c2bee4a2a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     42\u001b[0m             feed_data = {input_data: batch_input_[0], targets: batch_input_[1], \n\u001b[0;32m     43\u001b[0m                          target_sequence_length:seq_input_, source_sequence_length:seq_input_, lr:0.0001}\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m     \u001b[1;31m#             print('started')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda_python\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda_python\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda_python\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mD:\\anaconda_python\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda_python\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Ses = tf.Session(graph=train_graph)\n",
    "with tf.Session(graph=train_graph) as Ses:\n",
    "    print('session !')\n",
    "    file_name_queue = tf.train.string_input_producer(['inputs'])\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, single_x = reader.read(file_name_queue)\n",
    "    print('gogo~')\n",
    "    context_features = {\n",
    "        \"length\": tf.FixedLenFeature([], dtype=tf.int64)\n",
    "    }\n",
    "\n",
    "    sequence_features = {\n",
    "        \"tokens\": tf.FixedLenSequenceFeature([], dtype=tf.int64),\n",
    "        \"labels\": tf.FixedLenSequenceFeature([], dtype=tf.int64)\n",
    "    }\n",
    "\n",
    "    context_parsed, sequence_parsed = tf.parse_single_sequence_example(serialized=single_x,\n",
    "                                                                       context_features=context_features,\n",
    "                                                                       sequence_features=sequence_features\n",
    "                                                                      )\n",
    "    context_parsed['length'] = tf.cast(context_parsed['length'], tf.int32)\n",
    "    batch_seq, batch_inputs = tf.contrib.training.bucket_by_sequence_length(context_parsed['length'],\n",
    "                                                                            [sequence_parsed['tokens'], sequence_parsed['labels']],\n",
    "                                                                            batch_size=batch_size,\n",
    "                                                                            bucket_boundaries=[5,10],\n",
    "                                                                            dynamic_pad=True,\n",
    "                                                                            capacity=1000,\n",
    "                                                                            num_threads=1\n",
    "                                                                           )\n",
    "\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=Ses, coord=coord)\n",
    "    Ses.run(tf.global_variables_initializer())\n",
    "    epochs = 3\n",
    "    for epoch in range(1, epochs):\n",
    "        print('epoch', epoch)\n",
    "        i = 0\n",
    "        while True:\n",
    "\n",
    "            seq_input_, batch_input_ = Ses.run([batch_seq, batch_inputs])\n",
    "\n",
    "            feed_data = {input_data: batch_input_[0], targets: batch_input_[1], \n",
    "                         target_sequence_length:seq_input_, source_sequence_length:seq_input_, lr:0.0001}\n",
    "            _, loss = Ses.run([train_op, cost], feed_data)\n",
    "    #             print('started')\n",
    "            \n",
    "            i+=1\n",
    "            print('{} loss {} '.format(i, loss))\n",
    "            if i % 500 ==0:\n",
    "                checkpoint = './seq2seq_test/seq2seq_test.ckpt'\n",
    "                saver = tf.train.Saver()\n",
    "                saver.save(Ses, checkpoint)\n",
    "                print('model saved')\n",
    "#                 break"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

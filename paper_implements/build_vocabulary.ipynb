{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import datetime\n",
    "from glob import iglob\n",
    "import time\n",
    "# import attention\n",
    "from collections import deque\n",
    "import pickle\n",
    "import pyreader\n",
    "import numpy as np\n",
    "import sys\n",
    "import csv\n",
    "from batchmake import Batcher\n",
    "import astwalker\n",
    "import ast\n",
    "import tokenize\n",
    "import pprint\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datas= 'data_samples/'\n",
    "subdirectories = [os.path.join(datas,o) for o in os.listdir(datas) if os.path.isdir(os.path.join(datas,o))]\n",
    "python_files = [(directory, [y for x in os.walk(directory) for y in iglob(os.path.join(x[0], '*.py'))])\n",
    "                for directory in subdirectories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_source_tree(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        fstr = f.read()\n",
    "    fstr = fstr.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "    if not fstr.endswith('\\n'):\n",
    "        fstr += '\\n'\n",
    "    return fstr, ast.parse(fstr, filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5394\n"
     ]
    }
   ],
   "source": [
    "files = [f[1] for f in python_files]\n",
    "p_files = []\n",
    "for f in files:\n",
    "    for j in f :\n",
    "        p_files.append(j)\n",
    "print(len(p_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error when parsing  data_samples/django\\channels\\channels\\test\\base.py\n",
      "error when parsing  data_samples/django\\channels\\channels\\test\\liveserver.py\n",
      "error when parsing  data_samples/django\\django\\django\\shortcuts.py\n",
      "error when parsing  data_samples/django\\django\\django\\contrib\\contenttypes\\forms.py\n",
      "error when parsing  data_samples/django\\django\\django\\contrib\\gis\\db\\models\\fields.py\n",
      "error when parsing  data_samples/django\\django\\django\\contrib\\gis\\forms\\fields.py\n",
      "error when parsing  data_samples/django\\django\\django\\contrib\\gis\\geos\\libgeos.py\n",
      "error when parsing  data_samples/django\\django\\django\\contrib\\postgres\\forms\\array.py\n",
      "error when parsing  data_samples/django\\django\\django\\core\\checks\\security\\base.py\n",
      "error when parsing  data_samples/django\\django\\django\\core\\checks\\security\\csrf.py\n",
      "error when parsing  data_samples/django\\django\\django\\core\\checks\\security\\sessions.py\n",
      "error when parsing  data_samples/django\\django\\django\\core\\mail\\backends\\filebased.py\n",
      "error when parsing  data_samples/django\\django\\django\\core\\serializers\\base.py\n",
      "error when parsing  data_samples/django\\django\\django\\core\\serializers\\python.py\n",
      "error when parsing  data_samples/django\\django\\django\\core\\serializers\\xml_serializer.py\n",
      "error when parsing  data_samples/django\\django\\django\\core\\servers\\basehttp.py\n",
      "error when parsing  data_samples/django\\django\\django\\db\\backends\\oracle\\functions.py\n",
      "error when parsing  data_samples/django\\django\\django\\db\\models\\expressions.py\n",
      "error when parsing  data_samples/django\\django\\django\\db\\models\\fields\\related.py\n",
      "error when parsing  data_samples/django\\django\\django\\db\\models\\fields\\__init__.py\n",
      "error when parsing  data_samples/django\\django\\django\\db\\models\\functions\\base.py\n",
      "error when parsing  data_samples/django\\django\\django\\forms\\fields.py\n",
      "error when parsing  data_samples/django\\django\\django\\forms\\models.py\n",
      "error when parsing  data_samples/django\\django\\django\\template\\loader_tags.py\n",
      "error when parsing  data_samples/django\\django\\django\\test\\client.py\n",
      "error when parsing  data_samples/django\\django\\django\\test\\testcases.py\n",
      "error when parsing  data_samples/django\\django\\django\\utils\\archive.py\n",
      "error when parsing  data_samples/django\\django\\django\\utils\\datastructures.py\n",
      "error when parsing  data_samples/django\\django\\django\\views\\generic\\list.py\n",
      "error when parsing  data_samples/django\\django\\tests\\extra_regress\\models.py\n",
      "error when parsing  data_samples/django\\django\\tests\\forms_tests\\tests\\test_formsets.py\n",
      "error when parsing  data_samples/django\\django\\tests\\gis_tests\\test_data.py\n",
      "error when parsing  data_samples/django\\django\\tests\\gis_tests\\geos_tests\\test_geos.py\n",
      "error when parsing  data_samples/django\\django\\tests\\middleware\\test_security.py\n",
      "error when parsing  data_samples/django\\django\\tests\\multiple_database\\models.py\n",
      "error when parsing  data_samples/django\\django\\tests\\schema\\tests.py\n",
      "error when parsing  data_samples/django\\django\\tests\\staticfiles_tests\\cases.py\n",
      "error when parsing  data_samples/django\\django\\tests\\view_tests\\tests\\test_static.py\n",
      "error when parsing  data_samples/django-haystack\\django-haystack\\test_haystack\\test_generic_views.py\n",
      "error when parsing  data_samples/django-oscar\\django-oscar\\src\\oscar\\apps\\basket\\abstract_models.py\n",
      "error when parsing  data_samples/django-oscar\\django-oscar\\tests\\utils.py\n",
      "error when parsing  data_samples/google\\python-fire\\fire\\test_components_py3.py\n",
      "error when parsing  data_samples/GoogleCloudPlatform\\python-docs-samples\\appengine\\standard\\ndb\\async\\shopping_cart.py\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tf.contrib.keras.preprocessing.text.Tokenizer()\n",
    "for python_file in p_files:\n",
    "    \n",
    "    try:\n",
    "        source, tree = get_source_tree(python_file)\n",
    "        word_seq = tf.contrib.keras.preprocessing.text.text_to_word_sequence(source)\n",
    "        tokenizer.fit_on_texts(word_seq)\n",
    "    except:\n",
    "        print('error when parsing ',python_file)\n",
    "        continue\n",
    "# tokens = tokenize.generate_tokens(StringIO(source).readline)\n",
    "# print(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['import',\n",
       " 'os',\n",
       " 'import',\n",
       " 'web',\n",
       " 'detect',\n",
       " 'var1750',\n",
       " 'os',\n",
       " 'environ',\n",
       " \"'cloud\",\n",
       " 'storage',\n",
       " \"bucket'\",\n",
       " 'def',\n",
       " 'function961',\n",
       " 'arg2217',\n",
       " 'var948',\n",
       " \"'\",\n",
       " 'detect',\n",
       " 'resources',\n",
       " 'landmark',\n",
       " \"jpg'\",\n",
       " 'web',\n",
       " 'detect',\n",
       " 'report',\n",
       " 'web',\n",
       " 'detect',\n",
       " 'annotate',\n",
       " 'var948',\n",
       " 'var376',\n",
       " 'var1288',\n",
       " 'arg2217',\n",
       " 'readouterr',\n",
       " 'assert',\n",
       " \"'description\",\n",
       " 'palace',\n",
       " 'of',\n",
       " 'fine',\n",
       " 'arts',\n",
       " \"theatre'\",\n",
       " 'in',\n",
       " 'var376',\n",
       " 'def',\n",
       " 'function951',\n",
       " 'arg380',\n",
       " 'var4208',\n",
       " \"'gs\",\n",
       " 'vision',\n",
       " 'landmark',\n",
       " \"jpg'\",\n",
       " 'format',\n",
       " 'var1750',\n",
       " 'web',\n",
       " 'detect',\n",
       " 'report',\n",
       " 'web',\n",
       " 'detect',\n",
       " 'annotate',\n",
       " 'var4208',\n",
       " 'var4174',\n",
       " 'var598',\n",
       " 'arg380',\n",
       " 'readouterr',\n",
       " 'assert',\n",
       " \"'description\",\n",
       " 'palace',\n",
       " 'of',\n",
       " 'fine',\n",
       " 'arts',\n",
       " \"theatre'\",\n",
       " 'in',\n",
       " 'var4174',\n",
       " 'def',\n",
       " 'function530',\n",
       " 'arg930',\n",
       " 'web',\n",
       " 'detect',\n",
       " 'report',\n",
       " 'web',\n",
       " 'detect',\n",
       " 'annotate',\n",
       " \"'https\",\n",
       " 'goo',\n",
       " 'gl',\n",
       " \"x4qcb6'\",\n",
       " 'var2991',\n",
       " 'var3660',\n",
       " 'arg930',\n",
       " 'readouterr',\n",
       " 'assert',\n",
       " \"'https\",\n",
       " 'cloud',\n",
       " 'google',\n",
       " 'com',\n",
       " 'vision',\n",
       " \"'\",\n",
       " 'in',\n",
       " 'var2991']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(tokenizer.word_counts)\n",
    "# tokenizer.word_docs['import']\n",
    "# tokenizer.word_index['import']\n",
    "tokenizer.index_docs\n",
    "word_counts = tokenizer.word_counts\n",
    "word_counts = sorted(word_counts.items(), key=lambda x:x[1], reverse=True)\n",
    "word_counts = [word for word in word_counts if word[1] >10]\n",
    "word_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# print(word_counts)\n",
    "print(tokenizer.word_index[word_counts[7][0]])\n",
    "# tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7], [1]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['import','self'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',')\n",
    "    writer.writerow\n",
    "    for word in word_counts[:50000]:\n",
    "#         writer.writerow([word[0], tokenizer.word_index[word[0]], word[1]])\n",
    "        writer.writerow([word[0]])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

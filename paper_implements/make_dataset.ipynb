{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import datetime\n",
    "from glob import iglob\n",
    "import time\n",
    "# import attention\n",
    "from collections import deque\n",
    "import pickle\n",
    "import pyreader\n",
    "import numpy as np\n",
    "import sys\n",
    "import csv\n",
    "from batchmake import Batcher\n",
    "import astwalker\n",
    "import ast\n",
    "import tokenize\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pad_token, pad_id = \"§PAD§\", 0\n",
    "oov_token, oov_id = \"§OOV§\", 1\n",
    "indent_token = \"§<indent>§\"\n",
    "dedent_token = \"§<dedent>§\"\n",
    "number_token = \"§NUM§\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "481\n"
     ]
    }
   ],
   "source": [
    "datas= 'data_samples/'\n",
    "word_to_id='data_samples/mapping.map'\n",
    "with open(word_to_id, 'rb') as f:\n",
    "    word_to_id = pickle.load(f)\n",
    "inv_map = {v:k for k, v in word_to_id.items() }\n",
    "batch_data = datas+'output.txt.part0'\n",
    "# batch_data = datas+'preprocess.part0'\n",
    "with open(batch_data, 'rb') as f:\n",
    "    picked_data = pickle.load(f)\n",
    "    \n",
    "print(len(picked_data))\n",
    "train_proportion = 0.5\n",
    "valid_proportion = 0.2\n",
    "# word_to_id.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('vocab.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',')\n",
    "    writer.writerow(['\\\\n'])\n",
    "    for word in word_to_id:\n",
    "#         writer.writerow([word[0], tokenizer.word_index[word[0]], word[1]])\n",
    "        writer.writerow([word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "vocabulary = 'vocab.csv'\n",
    "vocabs = []\n",
    "with open(vocabulary, 'r', newline='', encoding='utf-8') as vocab:\n",
    "    words = csv.reader(vocab)\n",
    "    for word in words:\n",
    "        vocabs.append(word)\n",
    "print(len(vocabs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabs = [val for i, val in enumerate(word_to_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_to_id = {i[0]:w for w,i in enumerate(vocabs)}\n",
    "\n",
    "inv_map = {w:i for w,i in enumerate(vocabs)}\n",
    "# word_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subdirectories = [os.path.join(datas,o) for o in os.listdir(datas) if os.path.isdir(os.path.join(datas,o))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "python_files = [(directory, [y for x in os.walk(directory) for y in iglob(os.path.join(x[0], '*.py'))])\n",
    "                for directory in subdirectories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_split = int(len(python_files) * train_proportion)\n",
    "valid_split = train_split + int(len(python_files) * valid_proportion)\n",
    "train_files = []\n",
    "valid_files = []\n",
    "test_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for project in python_files[:train_split]:\n",
    "    train_files.extend([f[len(datas):] for f in project[1]])\n",
    "for project in python_files[train_split:valid_split]:\n",
    "    valid_files.extend([f[len(datas):] for f in project[1]])\n",
    "for project in python_files[valid_split:]:\n",
    "    test_files.extend([f[len(datas):] for f in project[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_file(fname, lst):\n",
    "    with open(os.path.join(datas, fname), \"w\") as write_file:\n",
    "        for f in lst:\n",
    "            print(f, file=write_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_to_file(\"train_files.txt\", train_files)\n",
    "write_to_file(\"valid_files.txt\", valid_files)\n",
    "write_to_file(\"test_files.txt\", test_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터셋을 train, valid, test셋으로 만든다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파일 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "python_files = [os.path.join(datas, f) for f in train_files]\n",
    "mapping = (lambda x: x) if word_to_id is None else (lambda x: word_to_id.get(x, oov_id))\n",
    "def get_source_tree(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        fstr = f.read()\n",
    "    fstr = fstr.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "    if not fstr.endswith('\\n'):\n",
    "        fstr += '\\n'\n",
    "    return fstr, ast.parse(fstr, filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(tokentype, tokenval):\n",
    "    if tokentype == tokenize.NUMBER:\n",
    "        return number_token\n",
    "\n",
    "    elif tokentype == tokenize.INDENT:\n",
    "        return indent_token\n",
    "\n",
    "    elif tokentype == tokenize.DEDENT:\n",
    "        return dedent_token\n",
    "\n",
    "    return tokenval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "definition_positions = []\n",
    "identifier_usage = []\n",
    "gen_def_positions = True\n",
    "error_data = 0\n",
    "data_file_name = []\n",
    "for filename in python_files:\n",
    "    try:\n",
    "        source, tree = get_source_tree(filename)\n",
    "        tokens = tokenize.generate_tokens(StringIO(source).readline)\n",
    "        data.append([(mapping(preprocess(tokenType, tokenVal)), start) for tokenType, tokenVal, start, _, _\n",
    "                     in tokens\n",
    "                     if tokenType != tokenize.COMMENT and\n",
    "                     not tokenVal.startswith(\"'''\") and\n",
    "                     not tokenVal.startswith('\"\"\"') and \n",
    "                     (tokenType == tokenize.DEDENT or tokenVal != \"\")])\n",
    "        data_file_name.append(filename)\n",
    "        if gen_def_positions:\n",
    "            walker = astwalker.ASTWalker()\n",
    "            walker.walk(tree)\n",
    "            definition_positions.append(walker.definition_positions)\n",
    "            identifier_usage.append(walker.name_usage)\n",
    "    except:\n",
    "        error_data+=1\n",
    "        continue\n",
    "#         print(\"Error when tokenizing %s: %s\" % (filename, sys.exc_info()[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전체 파일 ( python_files ) 중에서 에러난거 뺀데이터 (data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 4, 27, 27)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(python_files),error_data,len(data), len(definition_positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import sys\n",
      "import os\n",
      "sys.path.insert(0, os.path.abspath('..'))\n",
      "var3213 = ['sphinx.ext.autodoc', 'sphinx.ext.viewcode']\n",
      "var702 = ['_templates']\n",
      "var519 = '.rst'\n",
      "var2786 = 'index'\n",
      "var2133 = 'zhihu-py3'\n",
      "var389 = '2015, 7sDream'\n",
      "var1450 = '0.1'\n",
      "var1326 = '0.1'\n",
      "var2147 = ['_build']\n",
      "var2401 = 'sphinx'\n",
      "var2501 = (os.environ.get('READTHEDOCS', None) == 'True')\n",
      "if (not var2501):\n",
      "    import sphinx_rtd_theme\n",
      "    var2506 = 'sphinx_rtd_theme'\n",
      "    var722 = [sphinx_rtd_theme.get_html_theme_path()]\n",
      "else:\n",
      "    var2506 = 'default'\n",
      "var4586 = ['_static']\n",
      "var1086 = 'zhihu-py3doc'\n",
      "var1323 = {}\n",
      "var56 = [('index', 'zhihu-py3.tex', 'zhihu-py3 Documentation', '7sDream', 'manual')]\n",
      "var1033 = [('index', 'zhihu-py3', 'zhihu-py3 Documentation', ['7sDream'], 1)]\n",
      "var1893 = [('index', 'zhihu-py3', 'zhihu-py3 Documentation', '7sDream', 'zhihu-py3', 'One line description of project.', 'Miscellaneous')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dd,d=get_source_tree(data_file_name[1])\n",
    "print(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import sys \n",
      " import os \n",
      " sys . path . §OOV§ ( §NUM§ , os . path . abspath ( '..' ) ) \n",
      " var3213 = [ §OOV§ , §OOV§ ] \n",
      " var702 = [ §OOV§ ] \n",
      " var519 = §OOV§ \n",
      " var2786 = 'index' \n",
      " var2133 = 'zhihu-py3' \n",
      " var389 = §OOV§ \n",
      " var1450 = §OOV§ \n",
      " var1326 = §OOV§ \n",
      " var2147 = [ §OOV§ ] \n",
      " var2401 = §OOV§ \n",
      " var2501 = ( os . environ . get ( §OOV§ , None ) == 'True' ) \n",
      " if ( not var2501 ) : \n",
      " §<indent>§ import §OOV§ \n",
      " var2506 = §OOV§ \n",
      " var722 = [ §OOV§ . §OOV§ ( ) ] \n",
      " §<dedent>§ else : \n",
      " §<indent>§ var2506 = 'default' \n",
      " §<dedent>§ var4586 = [ §OOV§ ] \n",
      " var1086 = §OOV§ \n",
      " var1323 = { } \n",
      " var56 = [ ( 'index' , §OOV§ , §OOV§ , '7sDream' , §OOV§ ) ] \n",
      " var1033 = [ ( 'index' , 'zhihu-py3' , §OOV§ , [ '7sDream' ] , §NUM§ ) ] \n",
      " var1893 = [ ( 'index' , 'zhihu-py3' , §OOV§ , '7sDream' , 'zhihu-py3' , §OOV§ , §OOV§ ) ] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "token_to_str = []\n",
    "for tokend_data,position in data[1]:\n",
    "     token_to_str.append(inv_map[tokend_data])\n",
    "print(' '.join(token_to_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 원본데이터와 토큰데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type_max_rands = {\n",
    "    \"var\": 4750, \"function\": 2900, \"Class\": 440, \"attribute\": 2400, \"arg\": 2400\n",
    "}\n",
    "\n",
    "identifier_types = [key for key in type_max_rands]\n",
    "def_positions = [[[t[1] for t in fp if t[0] == k] for k in identifier_types] for fp in definition_positions]\n",
    "# ex) def_positions[101] #function, var, class, attribute, arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = []\n",
    "line_data_list = []\n",
    "for line_data in data:\n",
    "    for i, token in enumerate(line_data):\n",
    "#         print(token[0])\n",
    "#         inputs.append(token[0])\n",
    "        line_data_list.append(token[0])\n",
    "#         inputs.append(line_data_list)\n",
    "#         line_data_list=[]\n",
    "            \n",
    "#             print(token[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38661\n"
     ]
    }
   ],
   "source": [
    "# for row in line_data_list:\n",
    "#     print(row)\n",
    "print(len(line_data_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_dataset(sequence, labels):\n",
    "    ex = tf.train.SequenceExample()\n",
    "    sequence_length = len(sequence)\n",
    "    ex.context.feature['length'].int64_list.value.append(sequence_length)\n",
    "    fl_tokens = ex.feature_lists.feature_list['tokens']\n",
    "    fl_labels = ex.feature_lists.feature_list['labels']\n",
    "    for token, label in zip(sequence, labels):\n",
    "        fl_tokens.feature.add().int64_list.value.append(token)\n",
    "        fl_labels.feature.add().int64_list.value.append(label)\n",
    "    return ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence_length=5\n",
    "\n",
    "token = line_data_list[:sequence_length]\n",
    "label = line_data_list[1:sequence_length+1]\n",
    "ex = make_dataset(token,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open('inputs.csv', 'w',newline='', encoding='utf-8') as csvfile:\n",
    "#     spamwriter = csv.writer(csvfile)\n",
    "#     for row_line in inputs:\n",
    "#         spamwriter.writerow(row_line)\n",
    "# with open('inputs.csv', 'w',newline='', encoding='utf-8') as csvfile:\n",
    "#     spamwriter = csv.writer(csvfile)\n",
    "#     for row_line in inputs:\n",
    "#     spamwriter.writerow(line_data_list)\n",
    "with open('inputs', 'w',newline='', encoding='utf-8') as inputs:\n",
    "    writer = tf.python_io.TFRecordWriter(inputs.name)\n",
    "    for i in range(len(line_data_list)):\n",
    "        token = line_data_list[i* sequence_length: (i+1) * sequence_length]\n",
    "        label = line_data_list[(i* sequence_length)+1: (i+1) * sequence_length+1 ]\n",
    "        ex = make_dataset(token, label)\n",
    "        writer.write(ex.SerializeToString())\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 만들고 불러오는 예제."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_name_queue = tf.train.string_input_producer(['inputs'])\n",
    "reader = tf.TFRecordReader()\n",
    "_, data = reader.read(file_name_queue)\n",
    "\n",
    "context_features = {\n",
    "    \"length\": tf.FixedLenFeature([], dtype=tf.int64)\n",
    "}\n",
    "sequence_features = {\n",
    "    \"tokens\": tf.FixedLenSequenceFeature([], dtype=tf.int64),\n",
    "    \"labels\": tf.FixedLenSequenceFeature([], dtype=tf.int64)\n",
    "}\n",
    "\n",
    "context_parsed, sequence_parsed = tf.parse_single_sequence_example(serialized=data,\n",
    "                                 context_features=context_features,\n",
    "                                 sequence_features=sequence_features\n",
    "                                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "context_parsed['length'] = tf.cast(context_parsed['length'], tf.int32)\n",
    "batch_seq, batch_inputs = tf.contrib.training.bucket_by_sequence_length(context_parsed['length'],\n",
    "                                                                            [sequence_parsed['tokens'], sequence_parsed['labels']],\n",
    "                                                                            batch_size=128,\n",
    "                                                                            bucket_boundaries=[5,10],\n",
    "                                                                            dynamic_pad=True,\n",
    "                                                                            capacity=1000,\n",
    "                                                                            num_threads=1\n",
    "                                                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = tf.Session()\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=s, coord=coord)\n",
    "s.run(tf.global_variables_initializer())\n",
    "\n",
    "length_,tokens_ = s.run([batch_seq, batch_inputs])\n",
    "# print(length_, tokens_)\n",
    "# print(tokens_['tokens'], tokens_['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### target은 input+1 의 인덱스로 하고, sequence는 실행시간에 정함."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

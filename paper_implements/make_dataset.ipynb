{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import datetime\n",
    "from glob import iglob\n",
    "import time\n",
    "# import attention\n",
    "from collections import deque\n",
    "import pickle\n",
    "import pyreader\n",
    "import numpy as np\n",
    "import sys\n",
    "import csv\n",
    "from batchmake import Batcher\n",
    "import astwalker\n",
    "import ast\n",
    "import tokenize\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pad_token, pad_id = \"§PAD§\", 0\n",
    "oov_token, oov_id = \"§OOV§\", 1\n",
    "indent_token = \"§<indent>§\"\n",
    "dedent_token = \"§<dedent>§\"\n",
    "number_token = \"§NUM§\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "481\n"
     ]
    }
   ],
   "source": [
    "datas= 'data_samples/'\n",
    "word_to_id='data_samples/mapping.map'\n",
    "with open(word_to_id, 'rb') as f:\n",
    "    word_to_id = pickle.load(f)\n",
    "inv_map = {v:k for k, v in word_to_id.items() }\n",
    "# batch_data = datas+'output.txt.part0'\n",
    "batch_data = datas+'preprocess.part0'\n",
    "with open(batch_data, 'rb') as f:\n",
    "    picked_data = pickle.load(f)\n",
    "    \n",
    "print(len(picked_data))\n",
    "train_proportion = 0.5\n",
    "valid_proportion = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subdirectories = [os.path.join(datas,o) for o in os.listdir(datas) if os.path.isdir(os.path.join(datas,o))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "python_files = [(directory, [y for x in os.walk(directory) for y in iglob(os.path.join(x[0], '*.py'))])\n",
    "                for directory in subdirectories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_split = int(len(python_files) * train_proportion)\n",
    "valid_split = train_split + int(len(python_files) * valid_proportion)\n",
    "train_files = []\n",
    "valid_files = []\n",
    "test_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for project in python_files[:train_split]:\n",
    "    train_files.extend([f[len(datas):] for f in project[1]])\n",
    "for project in python_files[train_split:valid_split]:\n",
    "    valid_files.extend([f[len(datas):] for f in project[1]])\n",
    "for project in python_files[valid_split:]:\n",
    "    test_files.extend([f[len(datas):] for f in project[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_file(fname, lst):\n",
    "    with open(os.path.join(datas, fname), \"w\") as write_file:\n",
    "        for f in lst:\n",
    "            print(f, file=write_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_to_file(\"train_files.txt\", train_files)\n",
    "write_to_file(\"valid_files.txt\", valid_files)\n",
    "write_to_file(\"test_files.txt\", test_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터셋을 train, valid, test셋으로 만든다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파일 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "python_files = [os.path.join(datas, f) for f in train_files]\n",
    "mapping = (lambda x: x) if word_to_id is None else (lambda x: word_to_id.get(x, oov_id))\n",
    "def get_source_tree(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        fstr = f.read()\n",
    "    fstr = fstr.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "    if not fstr.endswith('\\n'):\n",
    "        fstr += '\\n'\n",
    "    return fstr, ast.parse(fstr, filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(tokentype, tokenval):\n",
    "    if tokentype == tokenize.NUMBER:\n",
    "        return number_token\n",
    "\n",
    "    elif tokentype == tokenize.INDENT:\n",
    "        return indent_token\n",
    "\n",
    "    elif tokentype == tokenize.DEDENT:\n",
    "        return dedent_token\n",
    "\n",
    "    return tokenval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "definition_positions = []\n",
    "identifier_usage = []\n",
    "gen_def_positions = True\n",
    "error_data = 0\n",
    "data_file_name = []\n",
    "for filename in python_files:\n",
    "    try:\n",
    "        source, tree = get_source_tree(filename)\n",
    "        tokens = tokenize.generate_tokens(StringIO(source).readline)\n",
    "        data.append([(mapping(preprocess(tokenType, tokenVal)), start) for tokenType, tokenVal, start, _, _\n",
    "                     in tokens\n",
    "                     if tokenType != tokenize.COMMENT and\n",
    "                     not tokenVal.startswith(\"'''\") and\n",
    "                     not tokenVal.startswith('\"\"\"') and \n",
    "                     (tokenType == tokenize.DEDENT or tokenVal != \"\")])\n",
    "        data_file_name.append(filename)\n",
    "        if gen_def_positions:\n",
    "            walker = astwalker.ASTWalker()\n",
    "            walker.walk(tree)\n",
    "            definition_positions.append(walker.definition_positions)\n",
    "            identifier_usage.append(walker.name_usage)\n",
    "    except:\n",
    "        error_data+=1\n",
    "        continue\n",
    "        print(\"Error when tokenizing %s: %s\" % (filename, sys.exc_info()[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전체 파일 ( python_files ) 중에서 에러난거 뺀데이터 (data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(534, 55, 479, 479)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(python_files),error_data,len(data), len(definition_positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from __future__ import unicode_literals\n",
      "import re\n",
      "import ast\n",
      "try:\n",
      "    from setuptools import setup\n",
      "except ImportError:\n",
      "    from distutils.core import setup\n",
      "\n",
      "def function2835():\n",
      "    with open('zhihu/__init__.py', 'rb') as var2990:\n",
      "        var4009 = re.search('__version__ = (.*)', var2990.read().decode('utf-8')).group(1)\n",
      "        if (var4009 is None):\n",
      "            raise RuntimeError('Cannot find version information')\n",
      "        return str(ast.literal_eval(var4009))\n",
      "with open('README.rst', 'rb') as var2178:\n",
      "    var2523 = var2178.read().decode('utf-8')\n",
      "var4384 = ['zhihu']\n",
      "var3646 = function2835()\n",
      "setup(name='zhihu-py3', version=var3646, keywords=['zhihu', 'network', 'spider', 'html'], description='Zhihu UNOFFICIAL API library in python3, with help of bs4, lxml, requests and html2text.', long_description=var2523, author='7sDream', author_email='didislover@gmail.com', license='MIT', url='https://github.com/7sDream/zhihu-py3', download_url='https://github.com/7sDream/zhihu-py3', install_requires=['beautifulsoup4', 'requests', 'html2text'], extras_require={'lxml': ['lxml'], }, packages=var4384, classifiers=['Development Status :: 3 - Alpha', 'Environment :: Web Environment', 'Intended Audience :: Developers', 'License :: OSI Approved :: MIT License', 'Operating System :: OS Independent', 'Programming Language :: Python :: 3', 'Topic :: Internet :: WWW/HTTP', 'Topic :: Software Development :: Libraries :: Python Modules'])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d , dd=get_source_tree(data_file_name[0])\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from __future__ import unicode_literals \n",
      " import re \n",
      " import §OOV§ \n",
      " try : \n",
      " §<indent>§ from §OOV§ import setup \n",
      " §<dedent>§ except ImportError : \n",
      " §<indent>§ from §OOV§ . §OOV§ import setup \n",
      " \n",
      " §<dedent>§ def function2835 ( ) : \n",
      " §<indent>§ with open ( §OOV§ , 'rb' ) as var2990 : \n",
      " §<indent>§ var4009 = re . search ( §OOV§ , var2990 . read ( ) . decode ( 'utf-8' ) ) . group ( §NUM§ ) \n",
      " if ( var4009 is None ) : \n",
      " §<indent>§ raise RuntimeError ( §OOV§ ) \n",
      " §<dedent>§ return str ( §OOV§ . §OOV§ ( var4009 ) ) \n",
      " §<dedent>§ §<dedent>§ with open ( §OOV§ , 'rb' ) as var2178 : \n",
      " §<indent>§ var2523 = var2178 . read ( ) . decode ( 'utf-8' ) \n",
      " §<dedent>§ var4384 = [ §OOV§ ] \n",
      " var3646 = function2835 ( ) \n",
      " setup ( name = 'zhihu-py3' , version = var3646 , §OOV§ = [ §OOV§ , §OOV§ , §OOV§ , 'html' ] , description = §OOV§ , §OOV§ = var2523 , author = '7sDream' , §OOV§ = §OOV§ , §OOV§ = §OOV§ , url = §OOV§ , §OOV§ = §OOV§ , §OOV§ = [ §OOV§ , §OOV§ , §OOV§ ] , §OOV§ = { §OOV§ : [ §OOV§ ] , } , packages = var4384 , §OOV§ = [ §OOV§ , §OOV§ , §OOV§ , §OOV§ , §OOV§ , §OOV§ , §OOV§ , §OOV§ ] ) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "token_to_str = []\n",
    "for tokend_data,position in data[0]:\n",
    "     token_to_str.append(inv_map[tokend_data])\n",
    "print(' '.join(token_to_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 원본데이터와 토큰데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type_max_rands = {\n",
    "    \"var\": 4750, \"function\": 2900, \"Class\": 440, \"attribute\": 2400, \"arg\": 2400\n",
    "}\n",
    "\n",
    "identifier_types = [key for key in type_max_rands]\n",
    "def_positions = [[[t[1] for t in fp if t[0] == k] for k in identifier_types] for fp in definition_positions]\n",
    "# ex) def_positions[101] #function, var, class, attribute, arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = []\n",
    "for line_data in data:\n",
    "    line_data_list = []\n",
    "    for i, token in enumerate(line_data):\n",
    "#         print(token[0])\n",
    "#         inputs.append(token[0])\n",
    "        line_data_list.append(token[0])\n",
    "        if i == 10:\n",
    "            inputs.append(line_data_list)\n",
    "            line_data_list=[]\n",
    "            \n",
    "#             print(token[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38, 455, 19, 3138, 2, 19, 63, 2, 19, 1, 2]\n",
      "[19, 62, 2, 19, 42, 2, 62, 5, 50, 5, 1]\n",
      "[38, 455, 19, 794, 6, 1720, 6, 3138, 2, 19, 273]\n",
      "[38, 455, 19, 794, 6, 1720, 6, 3138, 2, 19, 273]\n",
      "[38, 455, 19, 794, 6, 1720, 6, 3138, 2, 19, 273]\n",
      "[38, 455, 19, 794, 6, 1720, 6, 3138, 2, 19, 273]\n",
      "[19, 273, 2, 19, 63, 2, 38, 1008, 5, 637, 19]\n",
      "[38, 455, 19, 794, 6, 1720, 6, 3138, 2, 19, 273]\n",
      "[38, 455, 19, 794, 6, 1720, 6, 3138, 2, 19, 273]\n",
      "[19, 42, 2, 19, 1, 2, 2, 17, 3722, 3, 7018]\n",
      "[6568, 7, 4247, 2, 19, 42, 2, 19, 245, 2, 38]\n",
      "[38, 74, 19, 74, 2, 38, 5, 1, 19, 456, 2]\n",
      "[19, 1, 2, 1193, 7, 35, 1, 8, 1, 6, 1]\n",
      "[19, 148, 2, 38, 74, 19, 74, 2, 38, 5, 637]\n",
      "[19, 148, 2, 38, 5, 2216, 19, 1017, 2, 38, 5]\n",
      "[38, 5, 637, 19, 260, 2, 38, 2468, 19, 2599, 2]\n",
      "[38, 5, 637, 19, 22, 2, 38, 5, 2216, 19, 1017]\n",
      "[38, 5, 637, 19, 22, 2, 38, 5, 2216, 19, 1017]\n",
      "[56, 7618, 8, 2, 9, 1, 2, 2, 17, 65, 3]\n",
      "[19, 148, 2, 38, 5, 637, 19, 22, 2, 38, 5]\n",
      "[38, 5, 637, 19, 22, 2, 38, 5, 2216, 19, 1017]\n",
      "[19, 148, 2, 19, 126, 2, 38, 74, 19, 74, 2]\n",
      "[19, 126, 2, 38, 74, 19, 74, 2, 38, 5, 637]\n",
      "[38, 5, 1, 19, 3780, 2, 38, 5, 227, 19, 296]\n",
      "[19, 63, 2, 19, 62, 2, 19, 42, 2, 19, 3552]\n",
      "[19, 63, 2, 19, 343, 2, 19, 42, 2, 19, 1719]\n",
      "[19, 160, 6, 242, 6, 42, 2, 2, 2, 56, 536]\n",
      "[19, 160, 6, 242, 6, 42, 2, 2, 2, 56, 691]\n",
      "[6321, 7, 12, 2, 6519, 7, 12, 2, 1482, 7, 12]\n",
      "[38, 343, 19, 247, 6, 5047, 6, 1, 2, 19, 297]\n"
     ]
    }
   ],
   "source": [
    "for row in inputs[:30]:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('inputs.csv', 'w',newline='', encoding='utf-8') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile)\n",
    "    for row_line in inputs:\n",
    "        spamwriter.writerow(row_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### target은 input+1 의 인덱스로 하고, sequence는 실행시간에 정함."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

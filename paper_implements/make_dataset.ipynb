{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import datetime\n",
    "from glob import iglob\n",
    "import time\n",
    "# import attention\n",
    "from collections import deque\n",
    "import pickle\n",
    "import pyreader\n",
    "import numpy as np\n",
    "import sys\n",
    "import csv\n",
    "from batchmake import Batcher\n",
    "import astwalker\n",
    "import ast\n",
    "import tokenize\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pad_token, pad_id = \"§PAD§\", 0\n",
    "oov_token, oov_id = \"§OOV§\", 1\n",
    "indent_token = \"§<indent>§\"\n",
    "dedent_token = \"§<dedent>§\"\n",
    "number_token = \"§NUM§\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2515\n"
     ]
    }
   ],
   "source": [
    "datas= 'data_samples/'\n",
    "word_to_id='data_samples/mapping.map'\n",
    "with open(word_to_id, 'rb') as f:\n",
    "    word_to_id = pickle.load(f)\n",
    "inv_map = {v:k for k, v in word_to_id.items() }\n",
    "batch_data = datas+'output.txt.part0'\n",
    "# batch_data = datas+'preprocess.part0'\n",
    "with open(batch_data, 'rb') as f:\n",
    "    picked_data = pickle.load(f)\n",
    "    \n",
    "print(len(picked_data))\n",
    "train_proportion = 0.5\n",
    "valid_proportion = 0.2    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subdirectories = [os.path.join(datas,o) for o in os.listdir(datas) if os.path.isdir(os.path.join(datas,o))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "python_files = [(directory, [y for x in os.walk(directory) for y in iglob(os.path.join(x[0], '*.py'))])\n",
    "                for directory in subdirectories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_split = int(len(python_files) * train_proportion)\n",
    "valid_split = train_split + int(len(python_files) * valid_proportion)\n",
    "train_files = []\n",
    "valid_files = []\n",
    "test_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for project in python_files[:train_split]:\n",
    "    train_files.extend([f[len(datas):] for f in project[1]])\n",
    "for project in python_files[train_split:valid_split]:\n",
    "    valid_files.extend([f[len(datas):] for f in project[1]])\n",
    "for project in python_files[valid_split:]:\n",
    "    test_files.extend([f[len(datas):] for f in project[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_file(fname, lst):\n",
    "    with open(os.path.join(datas, fname), \"w\") as write_file:\n",
    "        for f in lst:\n",
    "            print(f, file=write_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_to_file(\"train_files.txt\", train_files)\n",
    "write_to_file(\"valid_files.txt\", valid_files)\n",
    "write_to_file(\"test_files.txt\", test_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터셋을 train, valid, test셋으로 만든다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파일 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "python_files = [os.path.join(datas, f) for f in train_files]\n",
    "mapping = (lambda x: x) if word_to_id is None else (lambda x: word_to_id.get(x, oov_id))\n",
    "def get_source_tree(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        fstr = f.read()\n",
    "    fstr = fstr.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "    if not fstr.endswith('\\n'):\n",
    "        fstr += '\\n'\n",
    "    return fstr, ast.parse(fstr, filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(tokentype, tokenval):\n",
    "    if tokentype == tokenize.NUMBER:\n",
    "        return number_token\n",
    "\n",
    "    elif tokentype == tokenize.INDENT:\n",
    "        return indent_token\n",
    "\n",
    "    elif tokentype == tokenize.DEDENT:\n",
    "        return dedent_token\n",
    "\n",
    "    return tokenval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "definition_positions = []\n",
    "identifier_usage = []\n",
    "gen_def_positions = True\n",
    "error_data = 0\n",
    "data_file_name = []\n",
    "for filename in python_files:\n",
    "    try:\n",
    "        source, tree = get_source_tree(filename)\n",
    "        tokens = tokenize.generate_tokens(StringIO(source).readline)\n",
    "        data.append([(mapping(preprocess(tokenType, tokenVal)), start) for tokenType, tokenVal, start, _, _\n",
    "                     in tokens\n",
    "                     if tokenType != tokenize.COMMENT and\n",
    "                     not tokenVal.startswith(\"'''\") and\n",
    "                     not tokenVal.startswith('\"\"\"') and \n",
    "                     (tokenType == tokenize.DEDENT or tokenVal != \"\")])\n",
    "        data_file_name.append(filename)\n",
    "        if gen_def_positions:\n",
    "            walker = astwalker.ASTWalker()\n",
    "            walker.walk(tree)\n",
    "            definition_positions.append(walker.definition_positions)\n",
    "            identifier_usage.append(walker.name_usage)\n",
    "    except:\n",
    "        error_data+=1\n",
    "        continue\n",
    "#         print(\"Error when tokenizing %s: %s\" % (filename, sys.exc_info()[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전체 파일 ( python_files ) 중에서 에러난거 뺀데이터 (data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3675, 41, 3634, 3634)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(python_files),error_data,len(data), len(definition_positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from __future__ import unicode_literals\n",
      "import binascii\n",
      "import os\n",
      "from django.contrib.auth.models import User\n",
      "from django.core.validators import MinLengthValidator\n",
      "from django.db import models\n",
      "from django.utils.encoding import python_2_unicode_compatible\n",
      "from django.utils import timezone\n",
      "\n",
      "\n",
      "@python_2_unicode_compatible\n",
      "class Class385(models.Model):\n",
      "    '\\n    An API token used for user authentication. This extends the stock model to allow each user to have multiple tokens.\\n    It also supports setting an expiration time and toggling write ability.\\n    '\n",
      "    var3736 = models.ForeignKey(User, related_name='tokens', on_delete=models.CASCADE)\n",
      "    var2751 = models.DateTimeField(auto_now_add=True)\n",
      "    var215 = models.DateTimeField(blank=True, null=True)\n",
      "    var2585 = models.CharField(max_length=40, unique=True, validators=[MinLengthValidator(40)])\n",
      "    var2200 = models.BooleanField(default=True, help_text='Permit create/update/delete operations using this key')\n",
      "    var1960 = models.CharField(max_length=100, blank=True)\n",
      "\n",
      "\n",
      "    class Class42:\n",
      "        var2956 = []\n",
      "\n",
      "    def __str__(self):\n",
      "        return '{} ({})'.format(self.var2585[(- 6):], self.var3736)\n",
      "\n",
      "    def function1564(self, *args, **kwargs):\n",
      "        if (not self.var2585):\n",
      "            self.attribute2024 = self.function1371()\n",
      "        return super(Class385, self).function1564(*args, data=kwargs)\n",
      "\n",
      "    def function1371(self):\n",
      "        return binascii.hexlify(os.urandom(20)).decode()\n",
      "\n",
      "    @property\n",
      "    def function120(self):\n",
      "        if ((self.var215 is None) or (timezone.now() < self.var215)):\n",
      "            return False\n",
      "        return True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d , dd=get_source_tree(data_file_name[200])\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from __future__ import unicode_literals \n",
      " import binascii \n",
      " import os \n",
      " from django . contrib . auth . models import User \n",
      " from django . core . validators import MinLengthValidator \n",
      " from django . db import models \n",
      " from django . utils . encoding import python_2_unicode_compatible \n",
      " from django . utils import timezone \n",
      " \n",
      " \n",
      " @ python_2_unicode_compatible \n",
      " class Class385 ( models . Model ) : \n",
      " §<indent>§ §OOV§ \n",
      " var3736 = models . ForeignKey ( User , related_name = §OOV§ , on_delete = models . CASCADE ) \n",
      " var2751 = models . DateTimeField ( auto_now_add = True ) \n",
      " var215 = models . DateTimeField ( blank = True , null = True ) \n",
      " var2585 = models . CharField ( max_length = §NUM§ , unique = True , validators = [ MinLengthValidator ( §NUM§ ) ] ) \n",
      " var2200 = models . BooleanField ( default = True , help_text = §OOV§ ) \n",
      " var1960 = models . CharField ( max_length = §NUM§ , blank = True ) \n",
      " \n",
      " \n",
      " class Class42 : \n",
      " §<indent>§ var2956 = [ ] \n",
      " \n",
      " §<dedent>§ def __str__ ( self ) : \n",
      " §<indent>§ return §OOV§ . format ( self . var2585 [ ( - §NUM§ ) : ] , self . var3736 ) \n",
      " \n",
      " §<dedent>§ def function1564 ( self , * args , ** kwargs ) : \n",
      " §<indent>§ if ( not self . var2585 ) : \n",
      " §<indent>§ self . §OOV§ = self . §OOV§ ( ) \n",
      " §<dedent>§ return super ( Class385 , self ) . function1564 ( * args , data = kwargs ) \n",
      " \n",
      " §<dedent>§ def §OOV§ ( self ) : \n",
      " §<indent>§ return binascii . §OOV§ ( os . §OOV§ ( §NUM§ ) ) . decode ( ) \n",
      " \n",
      " §<dedent>§ @ property \n",
      " def function120 ( self ) : \n",
      " §<indent>§ if ( ( self . var215 is None ) or ( timezone . now ( ) < self . var215 ) ) : \n",
      " §<indent>§ return False \n",
      " §<dedent>§ return True \n",
      " §<dedent>§ §<dedent>§\n"
     ]
    }
   ],
   "source": [
    "token_to_str = []\n",
    "for tokend_data,position in data[200]:\n",
    "     token_to_str.append(inv_map[tokend_data])\n",
    "print(' '.join(token_to_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 원본데이터와 토큰데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type_max_rands = {\n",
    "    \"var\": 4750, \"function\": 2900, \"Class\": 440, \"attribute\": 2400, \"arg\": 2400\n",
    "}\n",
    "\n",
    "identifier_types = [key for key in type_max_rands]\n",
    "def_positions = [[[t[1] for t in fp if t[0] == k] for k in identifier_types] for fp in definition_positions]\n",
    "# ex) def_positions[101] #function, var, class, attribute, arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = []\n",
    "line_data_list = []\n",
    "for line_data in data:\n",
    "    for i, token in enumerate(line_data):\n",
    "#         print(token[0])\n",
    "#         inputs.append(token[0])\n",
    "        line_data_list.append(token[0])\n",
    "#         inputs.append(line_data_list)\n",
    "#         line_data_list=[]\n",
    "            \n",
    "#             print(token[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2716373\n"
     ]
    }
   ],
   "source": [
    "# for row in line_data_list:\n",
    "#     print(row)\n",
    "print(len(line_data_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_dataset(sequence, labels):\n",
    "    ex = tf.train.SequenceExample()\n",
    "    sequence_length = len(sequence)\n",
    "    ex.context.feature['length'].int64_list.value.append(sequence_length)\n",
    "    fl_tokens = ex.feature_lists.feature_list['tokens']\n",
    "    fl_labels = ex.feature_lists.feature_list['labels']\n",
    "    for token, label in zip(sequence, labels):\n",
    "        fl_tokens.feature.add().int64_list.value.append(token)\n",
    "        fl_labels.feature.add().int64_list.value.append(label)\n",
    "    return ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence_length=5\n",
    "\n",
    "token = line_data_list[:sequence_length]\n",
    "label = line_data_list[1:sequence_length+1]\n",
    "ex = make_dataset(token,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('inputs.csv', 'w',newline='', encoding='utf-8') as csvfile:\n",
    "#     spamwriter = csv.writer(csvfile)\n",
    "#     for row_line in inputs:\n",
    "#         spamwriter.writerow(row_line)\n",
    "# with open('inputs.csv', 'w',newline='', encoding='utf-8') as csvfile:\n",
    "#     spamwriter = csv.writer(csvfile)\n",
    "#     for row_line in inputs:\n",
    "#     spamwriter.writerow(line_data_list)\n",
    "with open('inputs', 'w',newline='', encoding='utf-8') as inputs:\n",
    "    writer = tf.python_io.TFRecordWriter(inputs.name)\n",
    "    for i in range(len(line_data_list)):\n",
    "        token = line_data_list[i* sequence_length: (i+1) * sequence_length]\n",
    "        label = line_data_list[(i* sequence_length)+1: (i+1) * sequence_length+1 ]\n",
    "        ex = make_dataset(token, label)\n",
    "        writer.write(ex.SerializeToString())\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 만들고 불러오는 예제."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_name_queue = tf.train.string_input_producer(['inputs'])\n",
    "reader = tf.TFRecordReader()\n",
    "_, data = reader.read(file_name_queue)\n",
    "\n",
    "context_features = {\n",
    "    \"length\": tf.FixedLenFeature([], dtype=tf.int64)\n",
    "}\n",
    "sequence_features = {\n",
    "    \"tokens\": tf.FixedLenSequenceFeature([], dtype=tf.int64),\n",
    "    \"labels\": tf.FixedLenSequenceFeature([], dtype=tf.int64)\n",
    "}\n",
    "\n",
    "context_parsed, sequence_parsed = tf.parse_single_sequence_example(serialized=data,\n",
    "                                 context_features=context_features,\n",
    "                                 sequence_features=sequence_features\n",
    "                                )\n",
    "\n",
    "s = tf.Session()\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=s, coord=coord)\n",
    "s.run(tf.global_variables_initializer())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_,tokens_ = s.run([context_parsed,sequence_parsed])\n",
    "print(length_)\n",
    "print(tokens_['tokens'], tokens_['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### target은 input+1 의 인덱스로 하고, sequence는 실행시간에 정함."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

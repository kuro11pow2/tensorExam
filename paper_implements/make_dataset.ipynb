{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import datetime\n",
    "from glob import iglob\n",
    "import time\n",
    "# import attention\n",
    "from collections import deque\n",
    "import pickle\n",
    "import pyreader\n",
    "import numpy as np\n",
    "import sys\n",
    "import csv\n",
    "from batchmake import Batcher\n",
    "import astwalker\n",
    "import ast\n",
    "import tokenize\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pad_token, pad_id = \"§PAD§\", 0\n",
    "oov_token, oov_id = \"§OOV§\", 1\n",
    "indent_token = \"§<indent>§\"\n",
    "dedent_token = \"§<dedent>§\"\n",
    "number_token = \"§NUM§\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "481\n"
     ]
    }
   ],
   "source": [
    "datas= 'data_samples/'\n",
    "word_to_id='data_samples/mapping.map'\n",
    "with open(word_to_id, 'rb') as f:\n",
    "    word_to_id = pickle.load(f)\n",
    "inv_map = {v:k for k, v in word_to_id.items() }\n",
    "# batch_data = datas+'output.txt.part0'\n",
    "batch_data = datas+'preprocess.part0'\n",
    "with open(batch_data, 'rb') as f:\n",
    "    picked_data = pickle.load(f)\n",
    "    \n",
    "print(len(picked_data))\n",
    "train_proportion = 0.5\n",
    "valid_proportion = 0.2    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subdirectories = [os.path.join(datas,o) for o in os.listdir(datas) if os.path.isdir(os.path.join(datas,o))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "python_files = [(directory, [y for x in os.walk(directory) for y in iglob(os.path.join(x[0], '*.py'))])\n",
    "                for directory in subdirectories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_split = int(len(python_files) * train_proportion)\n",
    "valid_split = train_split + int(len(python_files) * valid_proportion)\n",
    "train_files = []\n",
    "valid_files = []\n",
    "test_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for project in python_files[:train_split]:\n",
    "    train_files.extend([f[len(datas):] for f in project[1]])\n",
    "for project in python_files[train_split:valid_split]:\n",
    "    valid_files.extend([f[len(datas):] for f in project[1]])\n",
    "for project in python_files[valid_split:]:\n",
    "    test_files.extend([f[len(datas):] for f in project[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_file(fname, lst):\n",
    "    with open(os.path.join(datas, fname), \"w\") as write_file:\n",
    "        for f in lst:\n",
    "            print(f, file=write_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_to_file(\"train_files.txt\", train_files)\n",
    "write_to_file(\"valid_files.txt\", valid_files)\n",
    "write_to_file(\"test_files.txt\", test_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터셋을 train, valid, test셋으로 만든다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파일 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "python_files = [os.path.join(datas, f) for f in train_files]\n",
    "mapping = (lambda x: x) if word_to_id is None else (lambda x: word_to_id.get(x, oov_id))\n",
    "def get_source_tree(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        fstr = f.read()\n",
    "    fstr = fstr.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "    if not fstr.endswith('\\n'):\n",
    "        fstr += '\\n'\n",
    "    return fstr, ast.parse(fstr, filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(tokentype, tokenval):\n",
    "    if tokentype == tokenize.NUMBER:\n",
    "        return number_token\n",
    "\n",
    "    elif tokentype == tokenize.INDENT:\n",
    "        return indent_token\n",
    "\n",
    "    elif tokentype == tokenize.DEDENT:\n",
    "        return dedent_token\n",
    "\n",
    "    return tokenval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "definition_positions = []\n",
    "identifier_usage = []\n",
    "gen_def_positions = True\n",
    "error_data = 0\n",
    "data_file_name = []\n",
    "for filename in python_files:\n",
    "    try:\n",
    "        source, tree = get_source_tree(filename)\n",
    "        tokens = tokenize.generate_tokens(StringIO(source).readline)\n",
    "        data.append([(mapping(preprocess(tokenType, tokenVal)), start) for tokenType, tokenVal, start, _, _\n",
    "                     in tokens\n",
    "                     if tokenType != tokenize.COMMENT and\n",
    "                     not tokenVal.startswith(\"'''\") and\n",
    "                     not tokenVal.startswith('\"\"\"') and \n",
    "                     (tokenType == tokenize.DEDENT or tokenVal != \"\")])\n",
    "        data_file_name.append(filename)\n",
    "        if gen_def_positions:\n",
    "            walker = astwalker.ASTWalker()\n",
    "            walker.walk(tree)\n",
    "            definition_positions.append(walker.definition_positions)\n",
    "            identifier_usage.append(walker.name_usage)\n",
    "    except:\n",
    "        error_data+=1\n",
    "        continue\n",
    "#         print(\"Error when tokenizing %s: %s\" % (filename, sys.exc_info()[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전체 파일 ( python_files ) 중에서 에러난거 뺀데이터 (data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(534, 55, 479, 479)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(python_files),error_data,len(data), len(definition_positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import dsz.lp.gui.terminal\n",
      "import sys, os.path, os\n",
      "import dsz.ui, dsz\n",
      "import ops.cmd\n",
      "\n",
      "def function2450():\n",
      "    var4004 = ('%s/DSky/Tools/%s-%s/BpfCompiler.exe' % (dsz.lp.GetResourcesDirectory(), 'i386', 'winnt'))\n",
      "    var2169 = dsz.ui.GetString('Please enter the filter to wish to use: ')\n",
      "    var3385 = '255.255.255.0'\n",
      "    try:\n",
      "        os.makedirs(os.path.join(dsz.lp.GetLogsDirectory(), 'CompiledBpf', 'src'))\n",
      "    except WindowsError:\n",
      "        pass\n",
      "    while 1:\n",
      "        var3814 = dsz.ui.GetString('What would you like to name the filter?')\n",
      "        var4257 = os.path.join(dsz.lp.GetLogsDirectory(), 'CompiledBpf', 'src', ('%s.fasm' % var3814))\n",
      "        var15 = os.path.join(dsz.lp.GetLogsDirectory(), 'CompiledBpf', ('%s.filt' % var3814))\n",
      "        if os.path.exists(var15):\n",
      "            dsz.ui.Echo(('%s already exists, please choose another name' % var15), dsz.ERROR)\n",
      "        else:\n",
      "            break\n",
      "    var1090 = open(var4257, 'w')\n",
      "    var1090.write(('filter:%s\\n' % var2169))\n",
      "    var1090.write(('netmask:%s' % var3385))\n",
      "    var1090.close()\n",
      "    var1359 = ops.cmd.getDszCommand('local run -redirect')\n",
      "    var1359.command = ('\"%s\" -i \"%s\" -o \"%s\"' % (var4004, var4257, var15))\n",
      "    var151 = var1359.execute()\n",
      "    var2397 = var151.processoutput[0].var2397.strip().replace('\\n\\n', '\\n')\n",
      "    if (var151.processstatus.status != 0):\n",
      "        dsz.ui.Echo('There was an error generating the filter', dsz.ERROR)\n",
      "        dsz.ui.Echo(var2397, dsz.ERROR)\n",
      "        return None\n",
      "    else:\n",
      "        dsz.ui.Echo('Compiled filter:', dsz.GOOD)\n",
      "        for var3052 in var2397.split('\\n'):\n",
      "            if var3052.startswith('0'):\n",
      "                dsz.ui.Echo(var3052)\n",
      "        dsz.ui.Echo(('Compiled filter file: %s' % var15), dsz.GOOD)\n",
      "    return True\n",
      "if (__name__ == '__main__'):\n",
      "    if (function2450() != True):\n",
      "        sys.exit((- 1))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d , dd=get_source_tree(data_file_name[200])\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import dsz . lp . §OOV§ . §OOV§ \n",
      " import sys , os . path , os \n",
      " import dsz . ui , dsz \n",
      " import ops . cmd \n",
      " \n",
      " def function2450 ( ) : \n",
      " §<indent>§ var4004 = ( §OOV§ % ( dsz . lp . GetResourcesDirectory ( ) , §OOV§ , 'winnt' ) ) \n",
      " var2169 = dsz . ui . GetString ( §OOV§ ) \n",
      " var3385 = §OOV§ \n",
      " try : \n",
      " §<indent>§ os . makedirs ( os . path . join ( dsz . lp . GetLogsDirectory ( ) , 'CompiledBpf' , 'src' ) ) \n",
      " §<dedent>§ except §OOV§ : \n",
      " §<indent>§ pass \n",
      " §<dedent>§ while §NUM§ : \n",
      " §<indent>§ var3814 = dsz . ui . GetString ( §OOV§ ) \n",
      " var4257 = os . path . join ( dsz . lp . GetLogsDirectory ( ) , 'CompiledBpf' , 'src' , ( §OOV§ % var3814 ) ) \n",
      " var15 = os . path . join ( dsz . lp . GetLogsDirectory ( ) , 'CompiledBpf' , ( §OOV§ % var3814 ) ) \n",
      " if os . path . exists ( var15 ) : \n",
      " §<indent>§ dsz . ui . Echo ( ( §OOV§ % var15 ) , dsz . ERROR ) \n",
      " §<dedent>§ else : \n",
      " §<indent>§ break \n",
      " §<dedent>§ §<dedent>§ var1090 = open ( var4257 , 'w' ) \n",
      " var1090 . write ( ( §OOV§ % var2169 ) ) \n",
      " var1090 . write ( ( §OOV§ % var3385 ) ) \n",
      " var1090 . close ( ) \n",
      " var1359 = ops . cmd . getDszCommand ( §OOV§ ) \n",
      " var1359 . command = ( §OOV§ % ( var4004 , var4257 , var15 ) ) \n",
      " var151 = var1359 . execute ( ) \n",
      " var2397 = var151 . §OOV§ [ §NUM§ ] . var2397 . strip ( ) . replace ( '\\n\\n' , '\\n' ) \n",
      " if ( var151 . §OOV§ . status != §NUM§ ) : \n",
      " §<indent>§ dsz . ui . Echo ( §OOV§ , dsz . ERROR ) \n",
      " dsz . ui . Echo ( var2397 , dsz . ERROR ) \n",
      " return None \n",
      " §<dedent>§ else : \n",
      " §<indent>§ dsz . ui . Echo ( §OOV§ , dsz . GOOD ) \n",
      " for var3052 in var2397 . split ( '\\n' ) : \n",
      " §<indent>§ if var3052 . startswith ( '0' ) : \n",
      " §<indent>§ dsz . ui . Echo ( var3052 ) \n",
      " §<dedent>§ §<dedent>§ dsz . ui . Echo ( ( §OOV§ % var15 ) , dsz . GOOD ) \n",
      " §<dedent>§ return True \n",
      " §<dedent>§ if ( __name__ == '__main__' ) : \n",
      " §<indent>§ if ( function2450 ( ) != True ) : \n",
      " §<indent>§ sys . exit ( ( - §NUM§ ) ) \n",
      " §<dedent>§ §<dedent>§\n"
     ]
    }
   ],
   "source": [
    "token_to_str = []\n",
    "for tokend_data,position in data[200]:\n",
    "     token_to_str.append(inv_map[tokend_data])\n",
    "print(' '.join(token_to_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 원본데이터와 토큰데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type_max_rands = {\n",
    "    \"var\": 4750, \"function\": 2900, \"Class\": 440, \"attribute\": 2400, \"arg\": 2400\n",
    "}\n",
    "\n",
    "identifier_types = [key for key in type_max_rands]\n",
    "def_positions = [[[t[1] for t in fp if t[0] == k] for k in identifier_types] for fp in definition_positions]\n",
    "# ex) def_positions[101] #function, var, class, attribute, arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = []\n",
    "line_data_list = []\n",
    "for line_data in data:\n",
    "    for i, token in enumerate(line_data):\n",
    "#         print(token[0])\n",
    "#         inputs.append(token[0])\n",
    "        line_data_list.append(token[0])\n",
    "#         inputs.append(line_data_list)\n",
    "#         line_data_list=[]\n",
    "            \n",
    "#             print(token[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "317308\n"
     ]
    }
   ],
   "source": [
    "# for row in line_data_list:\n",
    "#     print(row)\n",
    "print(len(line_data_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_dataset(sequence, labels):\n",
    "    ex = tf.train.SequenceExample()\n",
    "    sequence_length = len(sequence)\n",
    "    ex.context.feature['length'].int64_list.value.append(sequence_length)\n",
    "    fl_tokens = ex.feature_lists.feature_list['tokens']\n",
    "    fl_labels = ex.feature_lists.feature_list['labels']\n",
    "    for token, label in zip(sequence, labels):\n",
    "        fl_tokens.feature.add().int64_list.value.append(token)\n",
    "        fl_labels.feature.add().int64_list.value.append(label)\n",
    "    return ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length=5\n",
    "\n",
    "token = line_data_list[:sequence_length]\n",
    "label = line_data_list[1:sequence_length+1]\n",
    "ex = make_dataset(token,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('inputs.csv', 'w',newline='', encoding='utf-8') as csvfile:\n",
    "#     spamwriter = csv.writer(csvfile)\n",
    "#     for row_line in inputs:\n",
    "#         spamwriter.writerow(row_line)\n",
    "# with open('inputs.csv', 'w',newline='', encoding='utf-8') as csvfile:\n",
    "#     spamwriter = csv.writer(csvfile)\n",
    "#     for row_line in inputs:\n",
    "#     spamwriter.writerow(line_data_list)\n",
    "with open('inputs', 'w',newline='', encoding='utf-8') as inputs:\n",
    "    writer = tf.python_io.TFRecordWriter(inputs.name)\n",
    "    for i in range(len(line_data_list)):\n",
    "        token = line_data_list[i* sequence_length: (i+1) * sequence_length]\n",
    "        label = line_data_list[(i* sequence_length)+1: (i+1) * sequence_length+1 ]\n",
    "        ex = make_dataset(token, label)\n",
    "        writer.write(ex.SerializeToString())\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 만들고 불러오는 예제."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_queue = tf.train.string_input_producer(['inputs'])\n",
    "reader = tf.TFRecordReader()\n",
    "_, data = reader.read(file_name_queue)\n",
    "\n",
    "context_features = {\n",
    "    \"length\": tf.FixedLenFeature([], dtype=tf.int64)\n",
    "}\n",
    "sequence_features = {\n",
    "    \"tokens\": tf.FixedLenSequenceFeature([], dtype=tf.int64),\n",
    "    \"labels\": tf.FixedLenSequenceFeature([], dtype=tf.int64)\n",
    "}\n",
    "\n",
    "context_parsed, sequence_parsed = tf.parse_single_sequence_example(serialized=data,\n",
    "                                 context_features=context_features,\n",
    "                                 sequence_features=sequence_features\n",
    "                                )\n",
    "\n",
    "s = tf.Session()\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=s, coord=coord)\n",
    "s.run(tf.global_variables_initializer())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'length': 5}\n",
      "[ 3  1  4  2 10] [ 1  4  2 10 18]\n"
     ]
    }
   ],
   "source": [
    "length_,tokens_ = s.run([context_parsed,sequence_parsed])\n",
    "print(length_)\n",
    "print(tokens_['tokens'], tokens_['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### target은 input+1 의 인덱스로 하고, sequence는 실행시간에 정함."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

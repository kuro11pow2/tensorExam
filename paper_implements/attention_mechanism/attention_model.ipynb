{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'attention_rnn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-e294d2d36650>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mattention_rnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtfutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'attention_rnn'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import attention_rnn\n",
    "import rnn\n",
    "import tfutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ModelBase(object):\n",
    "    def __init__(self, is_training, config):\n",
    "        self.config = config\n",
    "        self.is_training = is_training\n",
    "        self.batch_size = batch_size = config.batch_size\n",
    "        self.seq_length = seq_length = config.seq_length\n",
    "        self.size = size = config.hidden_size\n",
    "        self.vocab_size = vocab_size = config.vocab_size\n",
    "\n",
    "        self._input_data = input_data = tf.placeholder(tf.int32, [seq_length, batch_size], name=\"inputs\")\n",
    "        self._targets = targets = tf.placeholder(tf.int32, [seq_length, batch_size], name=\"targets\")\n",
    "        self._actual_lengths = tf.placeholder(tf.int32, [batch_size], name=\"actual_lengths\")\n",
    "\n",
    "        cell = self.create_cell()\n",
    "\n",
    "        self._initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "        with tf.device('/cpu:0'):\n",
    "            self._embedding = embedding = tf.get_variable(\n",
    "                \"embedding\", [vocab_size, size],\n",
    "                trainable=config.embedding_trainable)\n",
    "\n",
    "            inputs = tf.gather(embedding, input_data)\n",
    "\n",
    "        if is_training and config.keep_prob < 1:\n",
    "            inputs = tf.nn.dropout(inputs, config.keep_prob)\n",
    "\n",
    "        self._logits, self._predict, self._loss, self._final_state = self.output_and_loss(cell, inputs)\n",
    "        self._cost = cost = tf.reduce_sum(self._loss) / batch_size\n",
    "\n",
    "        if not is_training:\n",
    "            return\n",
    "\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),\n",
    "                                          config.max_grad_norm)\n",
    "\n",
    "        self._lr = tf.Variable(0.0, trainable=False)\n",
    "        self._optimizer = tf.train.GradientDescentOptimizer(self.lr)\n",
    "        self._train_op = self._optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "        print(\"Constructing Basic Model\")\n",
    "\n",
    "    def assign_lr(self, session, lr_value):\n",
    "        session.run(tf.assign(self.lr, lr_value))\n",
    "\n",
    "    def create_cell(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def rnn(self, cell, inputs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def output_and_loss(self, cell, input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def lr(self):\n",
    "        return self._lr\n",
    "\n",
    "    @property\n",
    "    def input_data(self):\n",
    "        return self._input_data\n",
    "\n",
    "    @property\n",
    "    def targets(self):\n",
    "        return self._targets\n",
    "\n",
    "    @property\n",
    "    def initial_state(self):\n",
    "        return self._initial_state\n",
    "\n",
    "    @property\n",
    "    def cost(self):\n",
    "        return self._cost\n",
    "\n",
    "    @property\n",
    "    def final_state(self):\n",
    "        return self._final_state\n",
    "\n",
    "    @property\n",
    "    def train_op(self):\n",
    "        return self._train_op\n",
    "\n",
    "    @property\n",
    "    def optimizer(self):\n",
    "        return self._optimizer\n",
    "\n",
    "    @property\n",
    "    def logits(self):\n",
    "        return self._logits\n",
    "\n",
    "    @property\n",
    "    def predict(self):\n",
    "        return self._predict\n",
    "\n",
    "    @property\n",
    "    def loss(self):\n",
    "        return self._loss\n",
    "\n",
    "    @property\n",
    "    def actual_lengths(self):\n",
    "        return self._actual_lengths\n",
    "\n",
    "    @property\n",
    "    def is_attention_model(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def embedding_variable(self):\n",
    "        return self._embedding\n",
    "\n",
    "\n",
    "class BasicModel(ModelBase):\n",
    "    def __init__(self, is_training, config):\n",
    "        super(BasicModel, self).__init__(is_training, config)\n",
    "\n",
    "    def create_cell(self, size=None):\n",
    "        size = size or self.config.hidden_size\n",
    "\n",
    "        if self.is_training and self.config.keep_prob < 1:\n",
    "            lstm = tf.nn.rnn_cell.MultiRNNCell([tf.nn.rnn_cell.DropoutWrapper(\n",
    "                tf.nn.rnn_cell.BasicLSTMCell(size, forget_bias=1.0, state_is_tuple=True),\n",
    "                output_keep_prob=self.config.keep_prob)] * self.config.num_layers, state_is_tuple=True)\n",
    "\n",
    "        else:\n",
    "            lstm = tf.nn.rnn_cell.MultiRNNCell(\n",
    "                [tf.nn.rnn_cell.BasicLSTMCell(size, forget_bias=1.0, state_is_tuple=True)]\n",
    "                * self.config.num_layers, state_is_tuple=True)\n",
    "\n",
    "        return lstm\n",
    "\n",
    "    def rnn(self, cell, inputs):\n",
    "        return tf.nn.dynamic_rnn(cell, inputs, sequence_length=self.actual_lengths,\n",
    "                                 initial_state=self._initial_state)\n",
    "\n",
    "    def output_and_loss(self, cell, inputs):\n",
    "        output, state = self.rnn(cell, inputs)\n",
    "\n",
    "        output = tf.reshape(output, [-1, self.size])\n",
    "        softmax_w = tf.get_variable(\"softmax_w\", [self.size, self.vocab_size])\n",
    "        softmax_b = tf.get_variable(\"softmax_b\", [self.vocab_size])\n",
    "        logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "        predict = tf.nn.softmax(logits)\n",
    "\n",
    "        labels = tf.reshape(self.targets, [self.batch_size * self.seq_length, 1])\n",
    "\n",
    "        loss = tf.nn.sampled_softmax_loss(\n",
    "            tf.transpose(softmax_w), softmax_b, output, labels, self.config.num_samples, self.vocab_size) \\\n",
    "            if self.config.num_samples > 0 else \\\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(self._logits, tf.reshape(self.targets, [-1]))\n",
    "\n",
    "        return logits, predict, loss, state\n",
    "\n",
    "    @property\n",
    "    def is_attention_model(self):\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionModel(BasicModel):\n",
    "    def __init__(self, is_training, config):\n",
    "        self._num_attns = len(config.attention)\n",
    "        self._num_tasks = len(config.attention) + 1\n",
    "        self._masks = tf.placeholder(tf.bool,\n",
    "                                     [config.seq_length, config.batch_size, len(config.attention)], name=\"masks\")\n",
    "        self._max_attention = config.max_attention\n",
    "        self._lambda_type = config.lambda_type\n",
    "        self._min_tensor = tf.ones([config.batch_size, self._max_attention]) * -1e-38\n",
    "\n",
    "        super(AttentionModel, self).__init__(is_training, config)\n",
    "        print(\"Constructing Attention Model\")\n",
    "\n",
    "    @property\n",
    "    def masks(self):\n",
    "        return self._masks\n",
    "\n",
    "    @property\n",
    "    def num_tasks(self):\n",
    "        return self._num_tasks\n",
    "\n",
    "    def create_cell(self, size=None):\n",
    "        cell = super(AttentionModel, self).create_cell()\n",
    "        cell = attention_rnn.AttentionCell(cell, self._max_attention, self.size, self._num_attns,\n",
    "                                           self._lambda_type, self._min_tensor)\n",
    "        return cell\n",
    "\n",
    "    def rnn(self, cell, inputs):\n",
    "        inputs = tf.concat(2, [inputs,\n",
    "                               tf.cast(self._masks, tf.float32),\n",
    "                               tf.cast(tf.expand_dims(self.input_data, 2), tf.float32)])\n",
    "\n",
    "        #return rnn.dynamic_attention_rnn(cell, inputs, self._max_attention, self.num_tasks, self.batch_size,\n",
    "        #                                 sequence_length=self.actual_lengths, initial_state=self.initial_state)\n",
    "        return rnn.attention_rnn(cell, inputs, self.seq_length, self.initial_state, self.batch_size,\n",
    "                                 self.size, self._max_attention, self.num_tasks, sequence_length=self.actual_lengths)\n",
    "\n",
    "    def output_and_loss(self, cell, inputs):\n",
    "        def _attention_predict(alpha, attn_ids, batch_size, length, project_to):\n",
    "            alpha = tf.reshape(alpha, [-1], name=\"att_reshape\")\n",
    "            attn_ids = tf.reshape(tf.cast(attn_ids, tf.int64), [-1, 1], name=\"att_id_reshape\")\n",
    "            initial_indices = tf.expand_dims(tfutils.tile_vector(tf.cast(tf.range(batch_size), tf.int64), length), 1,\n",
    "                                             name=\"att_indices_expand\")\n",
    "            sp_indices = tf.concat(1, [initial_indices, attn_ids], name=\"att_indices_concat\")\n",
    "            attention_probs = tf.sparse_to_dense(sp_indices, [batch_size, project_to], alpha, validate_indices=False,\n",
    "                                                 name=\"att_sparse_to_dense\")\n",
    "            return attention_probs\n",
    "\n",
    "        def weighted_average(inputs, weights):\n",
    "            # inputs: (tasks, batch*t, vocab)\n",
    "            # weights: (tasks, batch*t)\n",
    "            # output: (batch*t, vocab)\n",
    "\n",
    "            weights = tf.expand_dims(weights, 2)  # (tasks, batch*t, 1)\n",
    "            weighted = inputs * weights  # (tasks, batch*t, vocab)\n",
    "            return tf.reduce_sum(weighted, [0])\n",
    "\n",
    "        output, alpha_tensor, attn_id_tensor, lmbda, state = self.rnn(cell, inputs)\n",
    "        output = tf.reshape(output, [-1, self.size], name=\"output_reshape\")\n",
    "        # (steps, batch, size) -> (steps*batch, size)\n",
    "\n",
    "        lmbda = tf.reshape(lmbda, [-1, self.num_tasks], name=\"lmbda_reshape\")  # (steps*batch, tasks)\n",
    "        task_weights = tf.transpose(lmbda)\n",
    "        alphas = [tf.reshape(alpha_tensor[:, :, t, :], [-1, self._max_attention]) for t in range(self.num_tasks-1)]\n",
    "        attn_ids = [tf.reshape(attn_id_tensor[:, :, t, :], [-1, self._max_attention]) for t in range(self.num_tasks-1)]\n",
    "        # (steps, batch, k) -> (steps*batch, k)\n",
    "\n",
    "        softmax_w = tf.get_variable(\"softmax_w\", [self.size, self.vocab_size])\n",
    "        softmax_b = tf.get_variable(\"softmax_b\", [self.vocab_size])\n",
    "\n",
    "        logits = tf.matmul(output, softmax_w, name=\"logits_matmul\") + softmax_b\n",
    "        standard_predict = tf.nn.softmax(logits, name=\"softmax\")  # (steps*batch, vocab)\n",
    "        attn_predict = [\n",
    "            _attention_predict(alpha,\n",
    "                               attn_id,\n",
    "                               self.batch_size * self.seq_length,\n",
    "                               self._max_attention, self.vocab_size)\n",
    "            for alpha, attn_id in zip(alphas, attn_ids)]  # [(steps*batch, vocab)]\n",
    "\n",
    "        prediction_tensor = tf.pack([standard_predict] + attn_predict)\n",
    "        predict = weighted_average(prediction_tensor, task_weights)\n",
    "\n",
    "        labels = tf.reshape(self.targets, [-1], name=\"label_reshape\")\n",
    "\n",
    "        lm_cross_entropy = tf.nn.sampled_softmax_loss(tf.transpose(softmax_w), softmax_b,\n",
    "                                                      output, tf.expand_dims(labels,1),\n",
    "                                                      self.config.num_samples, self.vocab_size)\n",
    "\n",
    "        attn_cross_entropies = [tfutils.cross_entropy_from_indices(labels, attn_id, alpha,\n",
    "                                                                   self.batch_size*self.seq_length, self._max_attention)\n",
    "                                for attn_id, alpha in zip(attn_ids, alphas)]\n",
    "\n",
    "        cross_entropies = tf.pack([lm_cross_entropy] + attn_cross_entropies) * task_weights\n",
    "        cross_entropy = tf.reduce_sum(cross_entropies, [0])\n",
    "\n",
    "        return logits, predict, cross_entropy, state\n",
    "\n",
    "    @property\n",
    "    def is_attention_model(self):\n",
    "        return True"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

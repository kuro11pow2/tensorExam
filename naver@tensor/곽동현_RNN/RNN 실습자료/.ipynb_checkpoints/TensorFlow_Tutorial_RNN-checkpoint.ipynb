{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TensorFlow Tutorial with RNN\n",
    "\n",
    "본 문서는 TensorFlow 를 사용하여 기초적인 RNN을 구현하고 실험하기 위한 실습 자료이다.\n",
    "\n",
    "The code and comments are written by Dong-Hyun Kwak (imcomking@gmail.com)\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks\n",
    "Recurrent Neural Networks, 이하 RNN는 다음과 같은 구조를 가진 모델이다. RNN은 히든레이어에서 자기자신을 향하는 weight를 이용해 데이터간의 시간관계를 학습할 수 있다. 이러한 문제들을 시계열 학습이라고 부르며, 기존에 널리 쓰이던 Hidden Markov Model을 뉴럴넷을 이용해 구현했다고 볼 수 있다.\n",
    "\n",
    "\n",
    "![](images/rnn.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/unroll_rnn.png)\n",
    "\n",
    "위의 구조는 1개의 Recurrent weight를 가진 hidden node이다. 이러한 hidden node들이 여러개를 모여 1개의 RNN layer를 형성하고, 이것이 다시 deep 하게 쌓이는 모델 또한 가능하다.(그러나 RNN은 deep 하게 쌓을 경우 학습이 쉽지 않다.)\n",
    "\n",
    "RNN의 경우 MLP나 CNN에 비해서 구현이 다소 복잡하다. 따라서 RNN은 TensorFlow에서 제공하는 추상화된 API 들을 이용해서 구현하는 것이 일반적이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/longterm.png)\n",
    "(출처: http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "\n",
    "- tf.contrib.rnn.BasicRNNCell : 기본 RNN cell이외에 매우 다양한 cell을 사용할 수 있다.\n",
    "https://www.tensorflow.org/api_guides/python/contrib.rnn#Core_RNN_Cells_for_use_with_TensorFlow_s_core_RNN_methods\n",
    "\n",
    "- tf.contrib.seq2seq.sequence_loss : N개의 sequence의 cross-entropy를 weighted sum 한 Loss. weight는 0or1의 masking용도로써 쓰인다.\n",
    "https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/sequence_loss\n",
    "\n",
    "### Simple toy seq2seq with RNN / LSTM\n",
    "<img src=\"images/rnn_seq2seq.jpg\">\n",
    "(이미지 출처: http://smile2x.tistory.com/archive/201605)\n",
    "(소스코드 출처: https://github.com/hunkim/DeepLearningZeroToAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print' (<ipython-input-1-7fc75f57b548>, line 38)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-7fc75f57b548>\"\u001b[1;36m, line \u001b[1;32m38\u001b[0m\n\u001b[1;33m    print \"outputs: \",outputs\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m Missing parentheses in call to 'print'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "idx2char = ['h', 'i', 'e', 'l', 'o']\n",
    "# Teach hello: hihell -> ihello\n",
    "x_data = [[0, 1, 0, 2, 3, 3]]   # hihell\n",
    "x_one_hot = [[[1, 0, 0, 0, 0],   # h 0 \n",
    "              [0, 1, 0, 0, 0],   # i 1\n",
    "              [1, 0, 0, 0, 0],   # h 0\n",
    "              [0, 0, 1, 0, 0],   # e 2\n",
    "              [0, 0, 0, 1, 0],   # l 3\n",
    "              [0, 0, 0, 1, 0]]]  # l 3\n",
    "\n",
    "y_data = [[1, 0, 2, 3, 3, 4]]    # ihello\n",
    "y_one_hot = [[[0, 1, 0, 0, 0],   # i 1\n",
    "              [1, 0, 0, 0, 0],   # h 0\n",
    "              [0, 0, 1, 0, 0],   # e 2\n",
    "              [0, 0, 0, 1, 0],   # l 3\n",
    "              [0, 0, 0, 1, 0],   # l 3\n",
    "              [0, 0, 0, 0, 1]]]  # o 4\n",
    "\n",
    "input_dim = 5  # one-hot size\n",
    "hidden_size = 10  # output from the LSTM. 5 to directly predict one-hot\n",
    "output_dim = 5\n",
    "batch_size = 1   # one sentence\n",
    "sequence_length = 6  # |ihello| == 6\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, sequence_length, input_dim])  # X one-hot\n",
    "Y = tf.placeholder(tf.float32, [None, sequence_length, output_dim])  # Y one-hot\n",
    "\n",
    "cell = tf.contrib.rnn.BasicRNNCell(num_units=hidden_size)\n",
    "#cell = tf.contrib.rnn.GRUCell(num_units=hidden_size)\n",
    "#cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_size)\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, initial_state=initial_state, dtype=tf.float32)\n",
    "print \"outputs: \",outputs\n",
    "\n",
    "outputs_flat = tf.reshape(outputs, [-1 , hidden_size])\n",
    "print \"outputs_flat: \",outputs_flat\n",
    "w = tf.Variable(tf.random_normal([hidden_size, output_dim]))\n",
    "b = tf.Variable(tf.random_normal([output_dim]))\n",
    "y_prediction = tf.nn.softmax(tf.matmul(outputs_flat, w) + b)\n",
    "print \"y_prediction:\", y_prediction\n",
    "y_prediction = tf.reshape(y_prediction, [-1 , sequence_length, output_dim])\n",
    "print \"y_prediction:\", y_prediction\n",
    "print Y\n",
    "\n",
    "y_label_pred = tf.argmax(y_prediction, 2)\n",
    "    \n",
    "loss = -tf.reduce_sum(Y*tf.log(y_prediction), name = 'cross_entropy')\n",
    "\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.1).minimize(loss)\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(1000):\n",
    "        l, _ = sess.run([loss, train], feed_dict={X: x_one_hot, Y: y_one_hot})\n",
    "        if i %100 ==0:\n",
    "            _y_prediction, _y_label_pred = sess.run([y_prediction, y_label_pred], feed_dict={X: x_one_hot})\n",
    "            print i, \"loss:\", l, \"prediction: \", _y_label_pred, \"true Y: \", y_data\n",
    "\n",
    "            # print char using dic\n",
    "            result_str = [idx2char[c] for c in np.squeeze(_y_label_pred)]\n",
    "            print \"Prediction str: \", ''.join(result_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Navigation Sequence Classification\n",
    "앞서 살펴본 단순한 toy 문제에서 한 층 나아가, 52739개의 웹페이지를 탐색한 데이터를 학습한다. 빠른 학습을 위해 전체 데이터 중에서 아주 일부(100만개중 1000개만 가져옴)만을 가져왔기 때문에 test accuracy는 극히 낮으므로, train accuracy이 높아지는 것만 확인하도록 하자.\n",
    "\n",
    "앞서 살펴본 예제는 seq2seq 이라는 형태의 RNN task이고, 본 예제는 전체 sequence를 보고서 어떤 class인지 예측하는 Sequence Classification 문제에 해당한다. 따라서 loss를 계산할 때 쓰이는 outputs가 어떻게 바뀌었는 지를 이해하는 것이 본 예제의 핵심이다.(마지막 step의 outputs만 가져옴)\n",
    "\n",
    "- tf.one_hot : 자동으로 one_hot encoding을 해주는 함수\n",
    "<br>https://www.tensorflow.org/api_docs/python/tf/one_hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  :  0.000217181 0.0\n",
      "Test accuracy: 0.000\n",
      "10  :  0.000216318 0.0\n",
      "20  :  0.000215133 0.0\n",
      "30  :  0.000213787 0.0\n",
      "40  :  0.000212321 0.0\n",
      "50  :  0.000210751 0.0\n",
      "Test accuracy: 0.000\n",
      "60  :  0.000209084 0.0\n",
      "70  :  0.000207321 0.0\n",
      "80  :  0.000205456 0.0\n",
      "90  :  0.000203478 0.0\n",
      "100  :  0.000201371 0.0\n",
      "Test accuracy: 0.000\n",
      "110  :  0.000199111 0.0\n",
      "120  :  0.000196676 0.00125\n",
      "130  :  0.000194038 0.00125\n",
      "140  :  0.000191169 0.0025\n",
      "150  :  0.000188032 0.0025\n",
      "Test accuracy: 0.000\n",
      "160  :  0.000184587 0.00625\n",
      "170  :  0.000180781 0.0175\n",
      "180  :  0.000176544 0.03125\n",
      "190  :  0.000171788 0.045\n",
      "200  :  0.000166384 0.0625\n",
      "Test accuracy: 0.000\n",
      "210  :  0.000160157 0.10875\n",
      "220  :  0.000152853 0.16\n",
      "230  :  0.000144084 0.21\n",
      "240  :  0.000133197 0.2875\n",
      "250  :  0.000119178 0.38375\n",
      "Test accuracy: 0.010\n",
      "260  :  0.000100746 0.50375\n",
      "270  :  7.69867e-05 0.6375\n",
      "280  :  4.94459e-05 0.7575\n",
      "290  :  2.52457e-05 0.86375\n",
      "300  :  1.06855e-05 0.95125\n",
      "Test accuracy: 0.025\n",
      "310  :  4.18085e-06 0.98375\n",
      "320  :  1.78268e-06 0.99875\n",
      "330  :  9.17186e-07 0.99875\n",
      "340  :  5.90027e-07 0.99875\n",
      "350  :  4.32424e-07 0.99875\n",
      "Test accuracy: 0.025\n",
      "360  :  3.37951e-07 1.0\n",
      "370  :  2.77822e-07 1.0\n",
      "380  :  2.38188e-07 1.0\n",
      "390  :  2.10426e-07 1.0\n",
      "400  :  1.89701e-07 1.0\n",
      "Test accuracy: 0.025\n",
      "410  :  1.7339e-07 1.0\n",
      "420  :  1.60043e-07 1.0\n",
      "430  :  1.48814e-07 1.0\n",
      "440  :  1.39175e-07 1.0\n",
      "450  :  1.30773e-07 1.0\n",
      "Test accuracy: 0.025\n",
      "460  :  1.23361e-07 1.0\n",
      "470  :  1.16762e-07 1.0\n",
      "480  :  1.10839e-07 1.0\n",
      "490  :  1.05487e-07 1.0\n",
      "500  :  1.00623e-07 1.0\n",
      "Test accuracy: 0.025\n",
      "510  :  9.61809e-08 1.0\n",
      "520  :  9.21056e-08 1.0\n",
      "530  :  8.83518e-08 1.0\n",
      "540  :  8.4882e-08 1.0\n",
      "550  :  8.16644e-08 1.0\n",
      "Test accuracy: 0.025\n",
      "560  :  7.86714e-08 1.0\n",
      "570  :  7.58805e-08 1.0\n",
      "580  :  7.32709e-08 1.0\n",
      "590  :  7.08254e-08 1.0\n",
      "600  :  6.8529e-08 1.0\n",
      "Test accuracy: 0.025\n",
      "610  :  6.63684e-08 1.0\n",
      "620  :  6.43314e-08 1.0\n",
      "630  :  6.24079e-08 1.0\n",
      "640  :  6.05886e-08 1.0\n",
      "650  :  5.88652e-08 1.0\n",
      "Test accuracy: 0.025\n",
      "660  :  5.72305e-08 1.0\n",
      "670  :  5.56775e-08 1.0\n",
      "680  :  5.42004e-08 1.0\n",
      "690  :  5.27937e-08 1.0\n",
      "700  :  5.14526e-08 1.0\n",
      "Test accuracy: 0.025\n",
      "710  :  5.01728e-08 1.0\n",
      "720  :  4.895e-08 1.0\n",
      "730  :  4.77805e-08 1.0\n",
      "740  :  4.66609e-08 1.0\n",
      "750  :  4.55882e-08 1.0\n",
      "Test accuracy: 0.025\n",
      "760  :  4.45596e-08 1.0\n",
      "770  :  4.35723e-08 1.0\n",
      "780  :  4.26238e-08 1.0\n",
      "790  :  4.17122e-08 1.0\n",
      "800  :  4.08354e-08 1.0\n",
      "Test accuracy: 0.025\n",
      "810  :  3.99914e-08 1.0\n",
      "820  :  3.91781e-08 1.0\n",
      "830  :  3.83944e-08 1.0\n",
      "840  :  3.76386e-08 1.0\n",
      "850  :  3.69089e-08 1.0\n",
      "Test accuracy: 0.025\n",
      "860  :  3.62044e-08 1.0\n",
      "870  :  3.55237e-08 1.0\n",
      "880  :  3.48656e-08 1.0\n",
      "890  :  3.42292e-08 1.0\n",
      "900  :  3.36133e-08 1.0\n",
      "Test accuracy: 0.025\n",
      "910  :  3.30171e-08 1.0\n",
      "920  :  3.24393e-08 1.0\n",
      "930  :  3.18794e-08 1.0\n",
      "940  :  3.13364e-08 1.0\n",
      "950  :  3.081e-08 1.0\n",
      "Test accuracy: 0.025\n",
      "960  :  3.02991e-08 1.0\n",
      "970  :  2.9803e-08 1.0\n",
      "980  :  2.93215e-08 1.0\n",
      "990  :  2.88535e-08 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "tf.reset_default_graph()\n",
    "\n",
    "train_x = np.loadtxt('1000_index_x.txt')\n",
    "train_y = np.loadtxt('1000_index_y.txt')\n",
    "\n",
    "nclasses = 52739 #len(items)\n",
    "nsteps = 4\n",
    "\n",
    "dimhidden = 100\n",
    "dimoutput = nclasses\n",
    "\n",
    "x = tf.placeholder(\"int32\", [None, nsteps])\n",
    "y = tf.placeholder(\"int32\", [None])\n",
    "x_one_hot = tf.one_hot(x, nclasses, on_value=1.0, off_value=0.0 , dtype='float')\n",
    "y_one_hot = tf.one_hot(y, nclasses, on_value=1.0, off_value=0.0, dtype='float')\n",
    "\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(dimhidden)\n",
    "outputs, states = tf.nn.dynamic_rnn(cell, x_one_hot,  dtype=tf.float32)\n",
    "last_output = outputs[:, -1, :]\n",
    "\n",
    "w = tf.Variable(tf.random_normal([dimhidden, dimoutput]))\n",
    "b = tf.Variable(tf.random_normal([dimoutput]))\n",
    "prediction = tf.nn.softmax(tf.matmul(last_output, w) + b)\n",
    "\n",
    "cost = -tf.reduce_mean( y_one_hot*tf.log(prediction+0.00000001))\n",
    "optm     = tf.train.AdamOptimizer().minimize(cost) \n",
    "accr     = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(prediction,1), tf.argmax(y_one_hot,1)), tf.float32))\n",
    "\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True)))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "epochs = 1000\n",
    "x_train, x_test, y_train, y_test = train_test_split(train_x, train_y, test_size=0.2, random_state=0)\n",
    "\n",
    "for i in range(epochs):\n",
    "    _, _cost, _accr = sess.run([optm, cost, accr], feed_dict={x: x_train, y: y_train})\n",
    "    if i%10 == 0 :\n",
    "        print i, \" : \", _cost , _accr\n",
    "    \n",
    "    if i%50 == 0 :\n",
    "        _cost, test_acc = sess.run([cost, accr], feed_dict={x: x_test, y: y_test})\n",
    "        print \"Test accuracy: %.3f\" % (test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Open-source TensorFlow Implementation\n",
    "\n",
    "아래 링크는 TensorFlow로 구현되어 공개된 여러 오픈소스 프로젝트들을 모아서 정리해 둔 페이지들이다. 이중 본인의 연구 분야와 관련있는 프로젝트를 clone, 수정하여 사용할 경우 개발시간을 크게 단축할 수 있다.\n",
    "\n",
    "https://github.com/tensorflow/models : Syntax Net, Magenta, Image2Txt\n",
    "<br>https://github.com/TensorFlowKR/awesome_tensorflow_implementations\n",
    "<br>https://github.com/aikorea/awesome-rl\n",
    "\n",
    "유명한 오픈소스 몇가지를 살펴보자."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
